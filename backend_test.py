import requests
import sys
import json
import time
from datetime import datetime

class CombatJudgingAPITester:
    def __init__(self, base_url="https://combatjudge.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.results = []

    def run_test(self, name, method, endpoint, expected_status, data=None, headers=None):
        """Run a single API test"""
        url = f"{self.api_url}/{endpoint}"
        if headers is None:
            headers = {'Content-Type': 'application/json'}

        self.tests_run += 1
        print(f"\nðŸ” Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, timeout=10)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=headers, timeout=10)
            elif method == 'PUT':
                # Handle form data for PUT requests (for round notes update)
                if isinstance(data, str) and 'note_text=' in data:
                    headers = {'Content-Type': 'application/x-www-form-urlencoded'}
                    response = requests.put(url, data=data, headers=headers, timeout=10)
                else:
                    response = requests.put(url, json=data, headers=headers, timeout=10)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, timeout=10)

            success = response.status_code == expected_status
            
            result = {
                'test': name,
                'success': success,
                'expected_status': expected_status,
                'actual_status': response.status_code,
                'url': url
            }
            
            if success:
                self.tests_passed += 1
                print(f"âœ… Passed - Status: {response.status_code}")
                try:
                    result['response'] = response.json()
                except:
                    result['response'] = response.text[:200]
            else:
                print(f"âŒ Failed - Expected {expected_status}, got {response.status_code}")
                print(f"   Response: {response.text[:200]}")
                result['error'] = response.text[:200]

            self.results.append(result)
            return success, response.json() if success and response.text else {}

        except Exception as e:
            print(f"âŒ Failed - Error: {str(e)}")
            result = {
                'test': name,
                'success': False,
                'error': str(e),
                'url': url
            }
            self.results.append(result)
            return False, {}

    def test_root_endpoint(self):
        """Test the root API endpoint"""
        return self.run_test("Root API Endpoint", "GET", "", 200)

    def test_status_endpoints(self):
        """Test status check endpoints"""
        # Test POST status
        status_data = {"client_name": f"test_client_{datetime.now().strftime('%H%M%S')}"}
        success, response = self.run_test("Create Status Check", "POST", "status", 200, status_data)
        
        if success:
            # Test GET status
            self.run_test("Get Status Checks", "GET", "status", 200)
        
        return success

    def test_calculate_score_endpoint(self):
        """Test the main scoring endpoint with sample data"""
        # Sample event data for testing
        sample_events = [
            {
                "bout_id": "test_bout_123",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "KD",
                "timestamp": 30.0,
                "metadata": {"severity": "hard"}
            },
            {
                "bout_id": "test_bout_123",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "ISS Head",
                "timestamp": 45.0,
                "metadata": {"power_strike": True}
            },
            {
                "bout_id": "test_bout_123",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Takedown",
                "timestamp": 60.0,
                "metadata": {"immediate_pass": True}
            },
            {
                "bout_id": "test_bout_123",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Submission Attempt",
                "timestamp": 90.0,
                "metadata": {"depth": "tight", "duration": 15}
            }
        ]

        score_request = {
            "bout_id": "test_bout_123",
            "round_num": 1,
            "events": sample_events,
            "round_duration": 300
        }

        success, response = self.run_test("Calculate Score", "POST", "calculate-score", 200, score_request)
        
        if success and response:
            print(f"   âœ… Score calculation successful")
            print(f"   Fighter 1 Score: {response.get('fighter1_score', {}).get('final_score', 'N/A')}")
            print(f"   Fighter 2 Score: {response.get('fighter2_score', {}).get('final_score', 'N/A')}")
            print(f"   Score Gap: {response.get('score_gap', 'N/A')}")
            print(f"   Gap Label: {response.get('gap_label', 'N/A')}")
            
            # Validate response structure
            required_fields = ['bout_id', 'round_num', 'fighter1_score', 'fighter2_score', 'score_gap', 'gap_label']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Validate subscores
            for fighter_key in ['fighter1_score', 'fighter2_score']:
                fighter_score = response.get(fighter_key, {})
                subscores = fighter_score.get('subscores', {})
                expected_subscores = ['KD', 'ISS', 'GCQ', 'TDQ', 'SUBQ', 'OC', 'AGG', 'RP', 'TSR']
                missing_subscores = [sub for sub in expected_subscores if sub not in subscores]
                
                if missing_subscores:
                    print(f"   âš ï¸  Missing subscores for {fighter_key}: {missing_subscores}")
                    return False
                    
                print(f"   {fighter_key} subscores: {subscores}")
        
        return success

    def test_edge_cases(self):
        """Test edge cases for the scoring system"""
        print("\nðŸ§ª Testing Edge Cases...")
        
        # Test with no events
        empty_request = {
            "bout_id": "test_bout_empty",
            "round_num": 1,
            "events": [],
            "round_duration": 300
        }
        
        success1, _ = self.run_test("Empty Events List", "POST", "calculate-score", 200, empty_request)
        
        # Test with invalid event type
        invalid_events = [{
            "bout_id": "test_bout_invalid",
            "round_num": 1,
            "fighter": "fighter1",
            "event_type": "INVALID_EVENT",
            "timestamp": 30.0,
            "metadata": {}
        }]
        
        invalid_request = {
            "bout_id": "test_bout_invalid",
            "round_num": 1,
            "events": invalid_events,
            "round_duration": 300
        }
        
        success2, _ = self.run_test("Invalid Event Type", "POST", "calculate-score", 200, invalid_request)
        
        return success1 and success2

    def test_event_counts_with_actual_frontend_types(self):
        """Test event counts with ACTUAL event types used by OperatorPanel frontend"""
        print("\nðŸ“Š Testing Event Counts with ACTUAL Frontend Event Types...")
        
        # Test data with ACTUAL frontend event types (with spaces!)
        test_events = [
            # Fighter 1 events: 3x "SS Head", 2x "SS Body", 1x "SS Leg", 2x "Takedown", 1x "CTRL_START", 1x "CTRL_STOP", 1x "Pass"
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Head", "timestamp": 10.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Head", "timestamp": 20.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Head", "timestamp": 30.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Body", "timestamp": 40.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Body", "timestamp": 50.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "SS Leg", "timestamp": 60.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "Takedown", "timestamp": 70.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "Takedown", "timestamp": 80.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "CTRL_START", "timestamp": 90.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "CTRL_STOP", "timestamp": 120.0, "metadata": {"duration": 30.0}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter1", "event_type": "Pass", "timestamp": 130.0, "metadata": {}},
            
            # Fighter 2 events: 2x "SS Head", 1x "KD", 1x "Submission Attempt"
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter2", "event_type": "SS Head", "timestamp": 15.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter2", "event_type": "SS Head", "timestamp": 25.0, "metadata": {}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter2", "event_type": "KD", "timestamp": 35.0, "metadata": {"severity": "hard"}},
            {"bout_id": "test_bout_actual", "round_num": 1, "fighter": "fighter2", "event_type": "Submission Attempt", "timestamp": 45.0, "metadata": {"depth": "tight"}}
        ]
        
        score_request = {
            "bout_id": "test_bout_actual",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Event Counts with Actual Frontend Types", "POST", "calculate-score", 200, score_request)
        
        if success and response:
            print(f"   âœ… Score calculation with ACTUAL frontend event types successful")
            
            # Verify fighter1_score has event_counts
            fighter1_score = response.get('fighter1_score', {})
            fighter1_event_counts = fighter1_score.get('event_counts', {})
            
            if not fighter1_event_counts:
                print(f"   âŒ Missing event_counts in fighter1_score")
                return False
            
            # Verify fighter1 event counts (as per test specification)
            expected_f1_counts = {
                "Significant Strikes": 6,  # 3 head + 2 body + 1 leg (NO KD for fighter1)
                "Grappling Control": 3,    # CTRL_START + CTRL_STOP + Pass
                "Aggression": 6,           # 3 head + 2 body + 1 leg
                "Damage": 0,               # No KD or Submission Attempt
                "Takedowns": 2             # 2 Takedown events
            }
            
            print(f"   Fighter 1 Event Counts: {fighter1_event_counts}")
            
            for category, expected_count in expected_f1_counts.items():
                actual_count = fighter1_event_counts.get(category, 0)
                if actual_count != expected_count:
                    print(f"   âŒ Fighter1 {category}: expected {expected_count}, got {actual_count}")
                    return False
                else:
                    print(f"   âœ… Fighter1 {category}: {actual_count} (correct)")
            
            # Verify fighter2_score has event_counts
            fighter2_score = response.get('fighter2_score', {})
            fighter2_event_counts = fighter2_score.get('event_counts', {})
            
            if not fighter2_event_counts:
                print(f"   âŒ Missing event_counts in fighter2_score")
                return False
            
            # Verify fighter2 event counts (as per test specification)
            expected_f2_counts = {
                "Significant Strikes": 3,  # 2 SS Head + 1 KD
                "Grappling Control": 0,    # No grappling control events
                "Aggression": 2,           # 2 SS Head (KD doesn't count for aggression)
                "Damage": 2,               # 1 KD + 1 Submission Attempt
                "Takedowns": 0             # No Takedown events
            }
            
            print(f"   Fighter 2 Event Counts: {fighter2_event_counts}")
            
            for category, expected_count in expected_f2_counts.items():
                actual_count = fighter2_event_counts.get(category, 0)
                if actual_count != expected_count:
                    print(f"   âŒ Fighter2 {category}: expected {expected_count}, got {actual_count}")
                    return False
                else:
                    print(f"   âœ… Fighter2 {category}: {actual_count} (correct)")
            
            # Verify subscores are still present
            fighter1_subscores = fighter1_score.get('subscores', {})
            fighter2_subscores = fighter2_score.get('subscores', {})
            
            expected_subscores = ['KD', 'ISS', 'GCQ', 'TDQ', 'SUBQ', 'OC', 'AGG', 'RP', 'TSR']
            
            for subscore in expected_subscores:
                if subscore not in fighter1_subscores:
                    print(f"   âŒ Missing subscore {subscore} in fighter1_score")
                    return False
                if subscore not in fighter2_subscores:
                    print(f"   âŒ Missing subscore {subscore} in fighter2_score")
                    return False
            
            print(f"   âœ… All subscores present for both fighters")
            
        else:
            return False
        
        # Test with empty events - verify counts are all 0
        empty_request = {
            "bout_id": "test_bout_empty_counts",
            "round_num": 1,
            "events": [],
            "round_duration": 300
        }
        
        success_empty, response_empty = self.run_test("Empty Events - Event Counts", "POST", "calculate-score", 200, empty_request)
        
        if success_empty and response_empty:
            print(f"   âœ… Empty events test successful")
            
            # Verify both fighters have event_counts with all zeros
            for fighter_key in ['fighter1_score', 'fighter2_score']:
                fighter_score = response_empty.get(fighter_key, {})
                event_counts = fighter_score.get('event_counts', {})
                
                if not event_counts:
                    print(f"   âŒ Missing event_counts in {fighter_key} for empty events")
                    return False
                
                for category, count in event_counts.items():
                    if count != 0:
                        print(f"   âŒ {fighter_key} {category}: expected 0, got {count} for empty events")
                        return False
                
                print(f"   âœ… {fighter_key} event counts all zero for empty events")
        else:
            return False
        
        print(f"   ðŸŽ‰ Event counts with ACTUAL frontend types test completed successfully!")
        return True

    def test_event_counts_in_scoring(self):
        """Test that calculate-score API returns event counts alongside subscores"""
        print("\nðŸ“Š Testing Event Counts in Scoring API...")
        
        # Test data with multiple events as specified in the request
        test_events = [
            # Fighter 1 events
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_HEAD", "timestamp": 10.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_HEAD", "timestamp": 20.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_HEAD", "timestamp": 30.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_HEAD", "timestamp": 40.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_HEAD", "timestamp": 50.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_BODY", "timestamp": 60.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_BODY", "timestamp": 70.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "SS_BODY", "timestamp": 80.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "TD", "timestamp": 90.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "TD", "timestamp": 100.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "CTRL_START", "timestamp": 110.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter1", "event_type": "CTRL_STOP", "timestamp": 140.0, "metadata": {"duration": 30.0}},
            
            # Fighter 2 events
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter2", "event_type": "SS_HEAD", "timestamp": 15.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter2", "event_type": "SS_HEAD", "timestamp": 25.0, "metadata": {}},
            {"bout_id": "test_bout_counts", "round_num": 1, "fighter": "fighter2", "event_type": "KD", "timestamp": 35.0, "metadata": {"severity": "hard"}}
        ]
        
        score_request = {
            "bout_id": "test_bout_counts",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Event Counts in Scoring", "POST", "calculate-score", 200, score_request)
        
        if success and response:
            print(f"   âœ… Score calculation with event counts successful")
            
            # Verify fighter1_score has event_counts
            fighter1_score = response.get('fighter1_score', {})
            fighter1_event_counts = fighter1_score.get('event_counts', {})
            
            if not fighter1_event_counts:
                print(f"   âŒ Missing event_counts in fighter1_score")
                return False
            
            # Verify fighter1 event counts
            expected_f1_counts = {
                "Significant Strikes": 8,  # 5 head + 3 body
                "Grappling Control": 2,    # CTRL_START + CTRL_STOP
                "Aggression": 8,           # Same as SS
                "Damage": 0,               # No KD or SUB_ATT
                "Takedowns": 2             # 2 TD events
            }
            
            print(f"   Fighter 1 Event Counts: {fighter1_event_counts}")
            
            for category, expected_count in expected_f1_counts.items():
                actual_count = fighter1_event_counts.get(category, 0)
                if actual_count != expected_count:
                    print(f"   âŒ Fighter1 {category}: expected {expected_count}, got {actual_count}")
                    return False
                else:
                    print(f"   âœ… Fighter1 {category}: {actual_count} (correct)")
            
            # Verify fighter2_score has event_counts
            fighter2_score = response.get('fighter2_score', {})
            fighter2_event_counts = fighter2_score.get('event_counts', {})
            
            if not fighter2_event_counts:
                print(f"   âŒ Missing event_counts in fighter2_score")
                return False
            
            # Verify fighter2 event counts
            expected_f2_counts = {
                "Significant Strikes": 3,  # 2 SS + 1 KD
                "Grappling Control": 0,    # No grappling events
                "Aggression": 2,           # 2 SS events (KD doesn't count for aggression)
                "Damage": 1,               # 1 KD
                "Takedowns": 0             # No TD events
            }
            
            print(f"   Fighter 2 Event Counts: {fighter2_event_counts}")
            
            for category, expected_count in expected_f2_counts.items():
                actual_count = fighter2_event_counts.get(category, 0)
                if actual_count != expected_count:
                    print(f"   âŒ Fighter2 {category}: expected {expected_count}, got {actual_count}")
                    return False
                else:
                    print(f"   âœ… Fighter2 {category}: {actual_count} (correct)")
            
            # Verify subscores are still present
            fighter1_subscores = fighter1_score.get('subscores', {})
            fighter2_subscores = fighter2_score.get('subscores', {})
            
            expected_subscores = ['KD', 'ISS', 'GCQ', 'TDQ', 'SUBQ', 'OC', 'AGG', 'RP', 'TSR']
            
            for subscore in expected_subscores:
                if subscore not in fighter1_subscores:
                    print(f"   âŒ Missing subscore {subscore} in fighter1_score")
                    return False
                if subscore not in fighter2_subscores:
                    print(f"   âŒ Missing subscore {subscore} in fighter2_score")
                    return False
            
            print(f"   âœ… All subscores present for both fighters")
            
        else:
            return False
        
        # Test with empty events - verify counts are all 0
        empty_request = {
            "bout_id": "test_bout_empty_counts",
            "round_num": 1,
            "events": [],
            "round_duration": 300
        }
        
        success_empty, response_empty = self.run_test("Empty Events - Event Counts", "POST", "calculate-score", 200, empty_request)
        
        if success_empty and response_empty:
            print(f"   âœ… Empty events test successful")
            
            # Verify both fighters have event_counts with all zeros
            for fighter_key in ['fighter1_score', 'fighter2_score']:
                fighter_score = response_empty.get(fighter_key, {})
                event_counts = fighter_score.get('event_counts', {})
                
                if not event_counts:
                    print(f"   âŒ Missing event_counts in {fighter_key} for empty events")
                    return False
                
                for category, count in event_counts.items():
                    if count != 0:
                        print(f"   âŒ {fighter_key} {category}: expected 0, got {count} for empty events")
                        return False
                
                print(f"   âœ… {fighter_key} event counts all zero for empty events")
        else:
            return False
        
        print(f"   ðŸŽ‰ Event counts in scoring API test completed successfully!")
        return True

    def test_shadow_judging_seed(self):
        """Test seeding the training library"""
        print("\nðŸŒ± Testing Shadow Judging - Seed Training Library...")
        success, response = self.run_test("Seed Training Library", "POST", "training-library/seed", 200)
        
        if success and response:
            print(f"   âœ… Seeded {response.get('count', 0)} training rounds")
            expected_count = 16  # Based on the sample data in server.py
            actual_count = response.get('count', 0)
            if actual_count != expected_count:
                print(f"   âš ï¸  Expected {expected_count} rounds, got {actual_count}")
                return False
        
        return success

    def test_shadow_judging_get_rounds(self):
        """Test getting all training rounds"""
        print("\nðŸ“š Testing Shadow Judging - Get Training Rounds...")
        success, response = self.run_test("Get Training Rounds", "GET", "training-library/rounds", 200)
        
        if success and response:
            rounds_count = len(response)
            print(f"   âœ… Retrieved {rounds_count} training rounds")
            
            # Verify structure of first round
            if rounds_count > 0:
                first_round = response[0]
                required_fields = ['id', 'event', 'fighters', 'roundNumber', 'summary', 'officialCard', 'type', 'createdAt']
                missing_fields = [field for field in required_fields if field not in first_round]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in round structure: {missing_fields}")
                    return False
                
                print(f"   âœ… Round structure validated")
                print(f"   Sample round: {first_round['event']} - {first_round['fighters']} (Round {first_round['roundNumber']})")
                
                # Store a round ID for later tests
                self.sample_round_id = first_round['id']
            else:
                print(f"   âš ï¸  No rounds returned")
                return False
        
        return success

    def test_shadow_judging_submit_scores(self):
        """Test submitting judge scores"""
        print("\nðŸ“Š Testing Shadow Judging - Submit Judge Scores...")
        
        if not hasattr(self, 'sample_round_id'):
            print("   âŒ No sample round ID available from previous test")
            return False
        
        # Test data for 3 judges with varying performance
        judges_data = [
            {
                "judgeId": "test-judge-1",
                "judgeName": "Alex Rodriguez",
                "scores": [
                    {"myScore": "10-9", "officialScore": "10-9", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True},
                    {"myScore": "10-8", "officialScore": "10-9", "mae": 1.0, "sensitivity108": False, "accuracy": 85.0, "match": False},
                    {"myScore": "10-9", "officialScore": "10-9", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True}
                ]
            },
            {
                "judgeId": "test-judge-2", 
                "judgeName": "Maria Santos",
                "scores": [
                    {"myScore": "10-9", "officialScore": "10-8", "mae": 1.0, "sensitivity108": False, "accuracy": 75.0, "match": False},
                    {"myScore": "10-9", "officialScore": "10-9", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True}
                ]
            },
            {
                "judgeId": "test-judge-3",
                "judgeName": "John Thompson", 
                "scores": [
                    {"myScore": "10-8", "officialScore": "10-8", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True},
                    {"myScore": "10-9", "officialScore": "10-8", "mae": 1.0, "sensitivity108": False, "accuracy": 80.0, "match": False},
                    {"myScore": "10-9", "officialScore": "10-9", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True},
                    {"myScore": "10-8", "officialScore": "10-8", "mae": 0.0, "sensitivity108": True, "accuracy": 100.0, "match": True}
                ]
            }
        ]
        
        all_success = True
        
        for judge_data in judges_data:
            judge_id = judge_data["judgeId"]
            judge_name = judge_data["judgeName"]
            
            for i, score_data in enumerate(judge_data["scores"]):
                submission_data = {
                    "judgeId": judge_id,
                    "judgeName": judge_name,
                    "roundId": self.sample_round_id,
                    **score_data
                }
                
                success, response = self.run_test(
                    f"Submit Score - {judge_name} #{i+1}", 
                    "POST", 
                    "training-library/submit-score", 
                    200, 
                    submission_data
                )
                
                if success and response:
                    print(f"   âœ… Score submitted for {judge_name}")
                    # Verify response structure
                    required_fields = ['id', 'judgeId', 'judgeName', 'roundId', 'myScore', 'officialScore', 'mae', 'sensitivity108', 'accuracy', 'match', 'timestamp']
                    missing_fields = [field for field in required_fields if field not in response]
                    
                    if missing_fields:
                        print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                        all_success = False
                else:
                    all_success = False
        
        return all_success

    def test_shadow_judging_judge_stats(self):
        """Test getting judge statistics"""
        print("\nðŸ“ˆ Testing Shadow Judging - Judge Statistics...")
        
        # Test stats for test-judge-1 (should have 3 attempts)
        success, response = self.run_test("Get Judge Stats - test-judge-1", "GET", "training-library/judge-stats/test-judge-1", 200)
        
        if success and response:
            print(f"   âœ… Stats retrieved for test-judge-1")
            
            # Verify response structure
            required_fields = ['judgeId', 'judgeName', 'totalAttempts', 'averageAccuracy', 'averageMAE', 'sensitivity108Rate', 'perfectMatches']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in stats response: {missing_fields}")
                return False
            
            # Verify calculations for test-judge-1
            # Expected: 3 attempts, avg accuracy = (100+85+100)/3 = 95, avg MAE = (0+1+0)/3 = 0.33, perfect matches = 2
            expected_attempts = 3
            expected_avg_accuracy = 95.0
            expected_perfect_matches = 2
            
            actual_attempts = response.get('totalAttempts', 0)
            actual_avg_accuracy = response.get('averageAccuracy', 0)
            actual_perfect_matches = response.get('perfectMatches', 0)
            
            print(f"   Total Attempts: {actual_attempts} (expected: {expected_attempts})")
            print(f"   Average Accuracy: {actual_avg_accuracy}% (expected: {expected_avg_accuracy}%)")
            print(f"   Perfect Matches: {actual_perfect_matches} (expected: {expected_perfect_matches})")
            
            if actual_attempts != expected_attempts:
                print(f"   âš ï¸  Incorrect total attempts")
                return False
            
            if abs(actual_avg_accuracy - expected_avg_accuracy) > 0.1:
                print(f"   âš ï¸  Incorrect average accuracy calculation")
                return False
            
            if actual_perfect_matches != expected_perfect_matches:
                print(f"   âš ï¸  Incorrect perfect matches count")
                return False
        
        # Test 404 for non-existent judge
        success_404, _ = self.run_test("Get Judge Stats - Non-existent", "GET", "training-library/judge-stats/non-existent-judge", 404)
        
        return success and success_404

    def test_shadow_judging_leaderboard(self):
        """Test getting the leaderboard"""
        print("\nðŸ† Testing Shadow Judging - Leaderboard...")
        
        success, response = self.run_test("Get Leaderboard", "GET", "training-library/leaderboard", 200)
        
        if success and response:
            leaderboard = response.get('leaderboard', [])
            print(f"   âœ… Leaderboard retrieved with {len(leaderboard)} judges")
            
            if len(leaderboard) > 0:
                # Verify structure of first entry
                first_entry = leaderboard[0]
                required_fields = ['judgeId', 'judgeName', 'totalAttempts', 'averageAccuracy', 'averageMAE', 'perfectMatches']
                missing_fields = [field for field in required_fields if field not in first_entry]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in leaderboard entry: {missing_fields}")
                    return False
                
                # Verify ranking order (should be sorted by accuracy descending)
                if len(leaderboard) > 1:
                    for i in range(len(leaderboard) - 1):
                        current_accuracy = leaderboard[i]['averageAccuracy']
                        next_accuracy = leaderboard[i + 1]['averageAccuracy']
                        if current_accuracy < next_accuracy:
                            print(f"   âš ï¸  Leaderboard not properly sorted by accuracy")
                            return False
                
                print(f"   âœ… Leaderboard structure and sorting validated")
                print(f"   Top judge: {first_entry['judgeName']} with {first_entry['averageAccuracy']}% accuracy")
            else:
                print(f"   âš ï¸  Empty leaderboard")
                return False
        
        return success

    def test_shadow_judging_complete_flow(self):
        """Test the complete Shadow Judging flow"""
        print("\nðŸŽ¯ Testing Complete Shadow Judging Flow...")
        
        # Step 1: Seed training library
        if not self.test_shadow_judging_seed():
            return False
        
        # Step 2: Get all rounds and verify
        if not self.test_shadow_judging_get_rounds():
            return False
        
        # Step 3: Submit scores for multiple judges
        if not self.test_shadow_judging_submit_scores():
            return False
        
        # Step 4: Get judge stats and verify calculations
        if not self.test_shadow_judging_judge_stats():
            return False
        
        # Step 5: Get leaderboard and verify ranking
        if not self.test_shadow_judging_leaderboard():
            return False
        
        print("   ðŸŽ‰ Complete Shadow Judging flow test passed!")
        return True

    def test_audit_create_log(self):
        """Test creating audit log entries"""
        print("\nðŸ” Testing Security & Audit - Create Audit Log...")
        
        # Test data for different audit log types
        audit_logs = [
            {
                "action_type": "score_calculation",
                "user_id": "judge-001",
                "user_name": "John Smith",
                "resource_type": "round_score",
                "resource_id": "bout_123_round_1",
                "action_data": {
                    "bout_id": "bout_123",
                    "round_num": 1,
                    "card": "10-9",
                    "winner": "fighter1",
                    "score_gap": 450.5
                },
                "ip_address": "192.168.1.100"
            },
            {
                "action_type": "flag_created",
                "user_id": "admin-001",
                "user_name": "Admin User",
                "resource_type": "discrepancy_flag",
                "resource_id": "flag_456",
                "action_data": {
                    "flag_type": "boundary_case",
                    "severity": "medium",
                    "bout_id": "bout_123"
                }
            },
            {
                "action_type": "profile_changed",
                "user_id": "judge-002",
                "user_name": "Maria Garcia",
                "resource_type": "tuning_profile",
                "resource_id": "profile_ufc_001",
                "action_data": {
                    "changes": ["weights.KD", "thresholds.threshold_10_8"],
                    "promotion": "UFC"
                }
            }
        ]
        
        created_log_ids = []
        all_success = True
        
        for i, log_data in enumerate(audit_logs):
            success, response = self.run_test(
                f"Create Audit Log #{i+1} - {log_data['action_type']}", 
                "POST", 
                "audit/log", 
                200, 
                log_data
            )
            
            if success and response:
                print(f"   âœ… Audit log created: {log_data['action_type']}")
                
                # Verify response structure
                if 'success' not in response or 'log_id' not in response:
                    print(f"   âš ï¸  Missing fields in response: expected 'success' and 'log_id'")
                    all_success = False
                else:
                    created_log_ids.append(response['log_id'])
                    print(f"   Log ID: {response['log_id']}")
            else:
                all_success = False
        
        # Store log IDs for later tests
        self.created_audit_log_ids = created_log_ids
        return all_success

    def test_audit_get_logs(self):
        """Test retrieving audit logs with filters"""
        print("\nðŸ“‹ Testing Security & Audit - Get Audit Logs...")
        
        # Test 1: Get all logs (no filters) - Owner access required
        success1, response1 = self.run_test("Get All Audit Logs", "GET", "audit/logs?judge_id=owner-001", 200)
        
        if success1 and response1:
            logs = response1.get('logs', [])
            count = response1.get('count', 0)
            immutable = response1.get('immutable', False)
            
            print(f"   âœ… Retrieved {count} audit logs")
            print(f"   WORM compliance: {immutable}")
            
            # Verify response structure
            required_fields = ['logs', 'count', 'immutable', 'message']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify log structure if logs exist
            if logs:
                first_log = logs[0]
                required_log_fields = ['id', 'timestamp', 'action_type', 'user_id', 'user_name', 
                                     'resource_type', 'resource_id', 'action_data', 'signature', 'immutable']
                missing_log_fields = [field for field in required_log_fields if field not in first_log]
                
                if missing_log_fields:
                    print(f"   âš ï¸  Missing fields in log structure: {missing_log_fields}")
                    return False
                
                print(f"   âœ… Log structure validated")
                print(f"   Sample log: {first_log['action_type']} by {first_log['user_name']}")
        
        # Test 2: Filter by action_type
        success2, response2 = self.run_test("Get Logs - Filter by action_type", "GET", "audit/logs?judge_id=owner-001&action_type=score_calculation", 200)
        
        if success2 and response2:
            filtered_logs = response2.get('logs', [])
            print(f"   âœ… Filtered by action_type: {len(filtered_logs)} logs")
            
            # Verify all logs have the correct action_type
            for log in filtered_logs:
                if log.get('action_type') != 'score_calculation':
                    print(f"   âš ï¸  Filter failed: found log with action_type '{log.get('action_type')}'")
                    return False
        
        # Test 3: Filter by user_id
        success3, response3 = self.run_test("Get Logs - Filter by user_id", "GET", "audit/logs?judge_id=owner-001&user_id=judge-001", 200)
        
        if success3 and response3:
            user_logs = response3.get('logs', [])
            print(f"   âœ… Filtered by user_id: {len(user_logs)} logs")
        
        # Test 4: Filter by resource_type
        success4, response4 = self.run_test("Get Logs - Filter by resource_type", "GET", "audit/logs?judge_id=owner-001&resource_type=round_score", 200)
        
        if success4 and response4:
            resource_logs = response4.get('logs', [])
            print(f"   âœ… Filtered by resource_type: {len(resource_logs)} logs")
        
        # Test 5: Multiple filters combined
        success5, response5 = self.run_test("Get Logs - Multiple filters", "GET", "audit/logs?judge_id=owner-001&action_type=score_calculation&user_id=judge-001", 200)
        
        if success5 and response5:
            combined_logs = response5.get('logs', [])
            print(f"   âœ… Multiple filters: {len(combined_logs)} logs")
        
        return success1 and success2 and success3 and success4 and success5

    def test_audit_stats(self):
        """Test audit statistics endpoint"""
        print("\nðŸ“Š Testing Security & Audit - Audit Statistics...")
        
        success, response = self.run_test("Get Audit Statistics", "GET", "audit/stats?judge_id=owner-001", 200)
        
        if success and response:
            total_logs = response.get('total_logs', 0)
            by_action_type = response.get('by_action_type', [])
            top_users = response.get('top_users', [])
            immutable = response.get('immutable', False)
            worm_compliant = response.get('worm_compliant', False)
            
            print(f"   âœ… Statistics retrieved")
            print(f"   Total logs: {total_logs}")
            print(f"   Action types: {len(by_action_type)}")
            print(f"   Top users: {len(top_users)}")
            print(f"   WORM compliant: {worm_compliant}")
            
            # Verify response structure
            required_fields = ['total_logs', 'by_action_type', 'top_users', 'immutable', 'worm_compliant']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify action type breakdown structure
            if by_action_type:
                first_type = by_action_type[0]
                if 'type' not in first_type or 'count' not in first_type:
                    print(f"   âš ï¸  Invalid action type breakdown structure")
                    return False
                
                print(f"   Top action type: {first_type['type']} ({first_type['count']} logs)")
            
            # Verify top users structure
            if top_users:
                first_user = top_users[0]
                required_user_fields = ['user_id', 'user_name', 'count']
                missing_user_fields = [field for field in required_user_fields if field not in first_user]
                
                if missing_user_fields:
                    print(f"   âš ï¸  Missing fields in top users: {missing_user_fields}")
                    return False
                
                print(f"   Top user: {first_user['user_name']} ({first_user['count']} logs)")
            
            # Verify calculations are reasonable
            if by_action_type:
                total_from_breakdown = sum(item['count'] for item in by_action_type)
                if total_from_breakdown != total_logs:
                    print(f"   âš ï¸  Statistics mismatch: total_logs={total_logs}, breakdown sum={total_from_breakdown}")
                    return False
                
                print(f"   âœ… Statistics calculations verified")
        
        return success

    def test_audit_verify_signature(self):
        """Test audit log signature verification"""
        print("\nðŸ” Testing Security & Audit - Verify Signatures...")
        
        if not hasattr(self, 'created_audit_log_ids') or not self.created_audit_log_ids:
            print("   âŒ No audit log IDs available from previous test")
            return False
        
        all_success = True
        
        # Test verification for each created log
        for i, log_id in enumerate(self.created_audit_log_ids):
            success, response = self.run_test(
                f"Verify Signature - Log #{i+1}", 
                "GET", 
                f"audit/verify/{log_id}?judge_id=owner-001", 
                200
            )
            
            if success and response:
                valid = response.get('valid', False)
                signature = response.get('signature', '')
                computed_signature = response.get('computed_signature', '')
                message = response.get('message', '')
                
                print(f"   âœ… Signature verification for log {log_id}")
                print(f"   Valid: {valid}")
                print(f"   Message: {message}")
                
                # Verify response structure
                required_fields = ['valid', 'signature', 'computed_signature', 'message']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in verification response: {missing_fields}")
                    all_success = False
                
                # Signature should be valid for newly created logs
                if not valid:
                    print(f"   âš ï¸  Signature validation failed for log {log_id}")
                    print(f"   Original: {signature}")
                    print(f"   Computed: {computed_signature}")
                    all_success = False
                
                # Signatures should match
                if signature != computed_signature:
                    print(f"   âš ï¸  Signature mismatch for log {log_id}")
                    all_success = False
            else:
                all_success = False
        
        # Test 404 for non-existent log
        success_404, _ = self.run_test("Verify Signature - Non-existent Log", "GET", "audit/verify/non-existent-log-id?judge_id=owner-001", 404)
        
        return all_success and success_404

    def test_audit_export(self):
        """Test audit log export functionality"""
        print("\nðŸ“¤ Testing Security & Audit - Export Audit Logs...")
        
        success, response = self.run_test("Export Audit Logs", "GET", "audit/export?judge_id=owner-001", 200)
        
        if success and response:
            export_format = response.get('export_format', '')
            export_timestamp = response.get('export_timestamp', '')
            record_count = response.get('record_count', 0)
            logs = response.get('logs', [])
            immutable = response.get('immutable', False)
            note = response.get('note', '')
            
            print(f"   âœ… Export completed")
            print(f"   Format: {export_format}")
            print(f"   Record count: {record_count}")
            print(f"   Export timestamp: {export_timestamp}")
            print(f"   WORM compliance: {immutable}")
            
            # Verify response structure
            required_fields = ['export_format', 'export_timestamp', 'record_count', 'logs', 'immutable', 'note']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in export response: {missing_fields}")
                return False
            
            # Verify export metadata
            if not export_timestamp:
                print(f"   âš ï¸  Missing export timestamp")
                return False
            
            if record_count != len(logs):
                print(f"   âš ï¸  Record count mismatch: stated={record_count}, actual={len(logs)}")
                return False
            
            # Verify exported logs have complete data
            if logs:
                first_log = logs[0]
                required_log_fields = ['id', 'timestamp', 'action_type', 'user_id', 'user_name', 
                                     'resource_type', 'resource_id', 'action_data', 'signature', 'immutable']
                missing_log_fields = [field for field in required_log_fields if field not in first_log]
                
                if missing_log_fields:
                    print(f"   âš ï¸  Missing fields in exported log: {missing_log_fields}")
                    return False
                
                print(f"   âœ… Exported log structure validated")
            
            # Verify compliance note
            if "WORM" not in note:
                print(f"   âš ï¸  Export note should mention WORM compliance")
                return False
        
        return success

    def test_audit_integration_flow(self):
        """Test complete Security & Audit integration flow"""
        print("\nðŸ” Testing Complete Security & Audit Integration Flow...")
        
        # Step 1: Create multiple audit logs with different types and users
        print("   Step 1: Creating audit logs...")
        if not self.test_audit_create_log():
            return False
        
        # Step 2: Retrieve logs with various filters
        print("   Step 2: Testing log retrieval and filtering...")
        if not self.test_audit_get_logs():
            return False
        
        # Step 3: Verify signatures for all created logs
        print("   Step 3: Verifying cryptographic signatures...")
        if not self.test_audit_verify_signature():
            return False
        
        # Step 4: Check statistics calculations
        print("   Step 4: Validating statistics aggregation...")
        if not self.test_audit_stats():
            return False
        
        # Step 5: Export and verify exported data
        print("   Step 5: Testing export functionality...")
        if not self.test_audit_export():
            return False
        
        print("   ðŸŽ‰ Complete Security & Audit integration flow test passed!")
        print("   âœ… All audit logs have SHA-256 signatures")
        print("   âœ… WORM (Write Once Read Many) compliance verified")
        print("   âœ… All CRUD operations working correctly")
        return True

    def test_judge_profile_create(self):
        """Test creating judge profiles"""
        print("\nðŸ‘¨â€âš–ï¸ Testing Judge Profile Management - Create Profiles...")
        
        # Test data for judge profiles
        judge_profiles = [
            {
                "judgeId": "JUDGE001",
                "judgeName": "Sarah Mitchell",
                "organization": "Nevada State Athletic Commission",
                "email": "sarah.mitchell@nsac.nv.gov",
                "certifications": ["MMA Level 1", "Boxing Level 2", "Muay Thai"]
            },
            {
                "judgeId": "JUDGE002", 
                "judgeName": "Carlos Rodriguez",
                "organization": "California State Athletic Commission",
                "email": "carlos.rodriguez@csac.ca.gov",
                "certifications": ["MMA Level 2", "BJJ Black Belt"]
            },
            {
                "judgeId": "owner-001",
                "judgeName": "Owner Judge",
                "organization": "System Administrator",
                "email": "owner@judgesync.com",
                "certifications": ["System Admin", "MMA Level 3"]
            }
        ]
        
        all_success = True
        created_judge_ids = []
        
        for i, profile_data in enumerate(judge_profiles):
            success, response = self.run_test(
                f"Create Judge Profile - {profile_data['judgeName']}", 
                "POST", 
                "judges", 
                200, 
                profile_data
            )
            
            if success and response:
                print(f"   âœ… Judge profile created: {profile_data['judgeName']}")
                
                # Verify response structure (create endpoint returns success message, not full profile)
                required_fields = ['success', 'message', 'judgeId']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    created_judge_ids.append(profile_data['judgeId'])
                    print(f"   Judge ID: {response['judgeId']}")
                    print(f"   Message: {response['message']}")
            else:
                all_success = False
        
        # Store created judge IDs for later tests
        self.created_judge_ids = created_judge_ids
        return all_success

    def test_judge_profile_get_with_stats(self):
        """Test getting judge profile with calculated stats"""
        print("\nðŸ“Š Testing Judge Profile Management - Get Profile with Stats...")
        
        if not hasattr(self, 'created_judge_ids') or not self.created_judge_ids:
            print("   âŒ No judge IDs available from previous test")
            return False
        
        all_success = True
        
        # Test getting each created judge profile
        for judge_id in self.created_judge_ids:
            success, response = self.run_test(
                f"Get Judge Profile - {judge_id}", 
                "GET", 
                f"judges/{judge_id}", 
                200
            )
            
            if success and response:
                print(f"   âœ… Profile retrieved for judge: {judge_id}")
                
                # Verify response structure
                required_fields = ['judgeId', 'judgeName', 'organization', 'email', 'certifications', 
                                 'createdAt', 'updatedAt', 'totalRoundsJudged', 'averageAccuracy', 'perfectMatches']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    print(f"   Judge Name: {response['judgeName']}")
                    print(f"   Organization: {response['organization']}")
                    print(f"   Total Rounds Judged: {response['totalRoundsJudged']}")
                    print(f"   Average Accuracy: {response['averageAccuracy']}%")
                    print(f"   Perfect Matches: {response['perfectMatches']}")
                    
                    # Verify stats are calculated from shadow judging data
                    # Stats should be calculated from training_scores collection
                    if judge_id in ['test-judge-1', 'test-judge-2', 'test-judge-3']:
                        # These judges should have stats from previous shadow judging tests
                        if response['totalRoundsJudged'] == 0:
                            print(f"   âš ï¸  Expected stats for {judge_id} but got 0 rounds")
                    else:
                        # New judges should have 0 stats initially
                        if response['totalRoundsJudged'] != 0:
                            print(f"   âš ï¸  New judge {judge_id} should have 0 rounds initially")
            else:
                all_success = False
        
        # Test 404 for non-existent judge
        success_404, _ = self.run_test("Get Judge Profile - Non-existent", "GET", "judges/NON_EXISTENT", 404)
        
        return all_success and success_404

    def test_judge_profile_update(self):
        """Test updating judge profiles"""
        print("\nâœï¸ Testing Judge Profile Management - Update Profiles...")
        
        if not hasattr(self, 'created_judge_ids') or not self.created_judge_ids:
            print("   âŒ No judge IDs available from previous test")
            return False
        
        # Test updating the first judge
        judge_id = self.created_judge_ids[0]
        
        # Update data
        update_data = {
            "judgeName": "Sarah Mitchell-Johnson",
            "organization": "Nevada State Athletic Commission - Senior Judge",
            "email": "sarah.mitchell.johnson@nsac.nv.gov"
        }
        
        success, response = self.run_test(
            f"Update Judge Profile - {judge_id}", 
            "PUT", 
            f"judges/{judge_id}", 
            200, 
            update_data
        )
        
        if success and response:
            print(f"   âœ… Judge profile updated: {judge_id}")
            
            # Verify response structure
            required_fields = ['judgeId', 'judgeName', 'organization', 'email', 'updatedAt']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify updates were applied
            if response['judgeName'] != update_data['judgeName']:
                print(f"   âš ï¸  Judge name not updated correctly")
                return False
            
            if response['organization'] != update_data['organization']:
                print(f"   âš ï¸  Organization not updated correctly")
                return False
            
            if response['email'] != update_data['email']:
                print(f"   âš ï¸  Email not updated correctly")
                return False
            
            print(f"   Updated Name: {response['judgeName']}")
            print(f"   Updated Organization: {response['organization']}")
            print(f"   Updated Email: {response['email']}")
            
            # Verify updatedAt timestamp changed
            # We can't easily test this without the original timestamp, but we can check it exists
            if not response.get('updatedAt'):
                print(f"   âš ï¸  updatedAt timestamp missing")
                return False
        
        # Test 404 for non-existent judge
        success_404, _ = self.run_test("Update Judge Profile - Non-existent", "PUT", "judges/NON_EXISTENT", 404, update_data)
        
        return success and success_404

    def test_judge_profile_history(self):
        """Test getting judge scoring history"""
        print("\nðŸ“ˆ Testing Judge Profile Management - Scoring History...")
        
        # Test history for judges that should have shadow judging data
        test_judges = ['test-judge-1', 'test-judge-2', 'test-judge-3']
        
        all_success = True
        
        for judge_id in test_judges:
            success, response = self.run_test(
                f"Get Judge History - {judge_id}", 
                "GET", 
                f"judges/{judge_id}/history", 
                200
            )
            
            if success and response:
                print(f"   âœ… History retrieved for judge: {judge_id}")
                
                # Verify response structure
                required_fields = ['judgeId', 'history', 'stats']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                    continue
                
                submissions = response.get('history', [])
                stats = response.get('stats', {})
                
                print(f"   Submissions count: {len(submissions)}")
                
                # Verify submissions structure if any exist
                if submissions:
                    first_submission = submissions[0]
                    required_submission_fields = ['id', 'roundId', 'myScore', 'officialScore', 'mae', 'accuracy', 'match', 'timestamp']
                    missing_submission_fields = [field for field in required_submission_fields if field not in first_submission]
                    
                    if missing_submission_fields:
                        print(f"   âš ï¸  Missing fields in submission: {missing_submission_fields}")
                        all_success = False
                    else:
                        print(f"   Sample submission: {first_submission['myScore']} vs {first_submission['officialScore']} (Accuracy: {first_submission['accuracy']}%)")
                        
                        # Verify submissions are sorted by timestamp (newest first)
                        if len(submissions) > 1:
                            for i in range(len(submissions) - 1):
                                current_time = submissions[i]['timestamp']
                                next_time = submissions[i + 1]['timestamp']
                                # Note: This is a simple string comparison, might need datetime parsing for proper validation
                                if current_time < next_time:
                                    print(f"   âš ï¸  Submissions not sorted by timestamp (newest first)")
                                    all_success = False
                                    break
                
                # Verify stats structure
                required_stats_fields = ['totalAttempts', 'averageAccuracy', 'averageMAE', 'perfectMatches']
                missing_stats_fields = [field for field in required_stats_fields if field not in stats]
                
                if missing_stats_fields:
                    print(f"   âš ï¸  Missing fields in stats: {missing_stats_fields}")
                    all_success = False
                else:
                    print(f"   Stats - Total: {stats['totalAttempts']}, Accuracy: {stats['averageAccuracy']}%, Perfect: {stats['perfectMatches']}")
            else:
                all_success = False
        
        # Test with limit parameter
        if test_judges:
            judge_id = test_judges[0]
            success_limit, response_limit = self.run_test(
                f"Get Judge History with Limit - {judge_id}", 
                "GET", 
                f"judges/{judge_id}/history?limit=2", 
                200
            )
            
            if success_limit and response_limit:
                submissions = response_limit.get('history', [])
                if len(submissions) > 2:
                    print(f"   âš ï¸  Limit parameter not working: expected max 2, got {len(submissions)}")
                    all_success = False
                else:
                    print(f"   âœ… Limit parameter working: {len(submissions)} submissions returned")
        
        # Test judge with no history (should return empty but valid structure)
        if hasattr(self, 'created_judge_ids') and self.created_judge_ids:
            new_judge_id = self.created_judge_ids[0]
            success_empty, response_empty = self.run_test(
                f"Get Judge History - No History ({new_judge_id})", 
                "GET", 
                f"judges/{new_judge_id}/history", 
                200
            )
            
            if success_empty and response_empty:
                submissions = response_empty.get('history', [])
                if len(submissions) != 0:
                    print(f"   âš ï¸  New judge should have empty history, got {len(submissions)} submissions")
                    all_success = False
                else:
                    print(f"   âœ… New judge has empty history as expected")
        
        return all_success

    def test_audit_owner_access_control(self):
        """Test owner-only access control for audit logs"""
        print("\nðŸ”’ Testing Audit Logs - Owner Access Control...")
        
        all_success = True
        
        # Test 1: Owner access (should work)
        success_owner, response_owner = self.run_test(
            "Audit Logs - Owner Access", 
            "GET", 
            "audit/logs?judge_id=owner-001", 
            200
        )
        
        if success_owner and response_owner:
            print(f"   âœ… Owner access granted successfully")
            logs = response_owner.get('logs', [])
            print(f"   Retrieved {len(logs)} logs for owner")
        else:
            all_success = False
        
        # Test 2: Non-owner access (should return 403)
        success_non_owner, response_non_owner = self.run_test(
            "Audit Logs - Non-Owner Access", 
            "GET", 
            "audit/logs?judge_id=JUDGE001", 
            403
        )
        
        if success_non_owner:
            print(f"   âœ… Non-owner access correctly denied (403)")
        else:
            print(f"   âŒ Non-owner access should return 403")
            all_success = False
        
        # Test 3: Owner access to audit stats
        success_owner_stats, response_owner_stats = self.run_test(
            "Audit Stats - Owner Access", 
            "GET", 
            "audit/stats?judge_id=owner-001", 
            200
        )
        
        if success_owner_stats and response_owner_stats:
            print(f"   âœ… Owner access to stats granted")
        else:
            all_success = False
        
        # Test 4: Non-owner access to audit stats (should return 403)
        success_non_owner_stats, response_non_owner_stats = self.run_test(
            "Audit Stats - Non-Owner Access", 
            "GET", 
            "audit/stats?judge_id=JUDGE001", 
            403
        )
        
        if success_non_owner_stats:
            print(f"   âœ… Non-owner access to stats correctly denied (403)")
        else:
            all_success = False
        
        # Test 5: Owner access to verify endpoint
        if hasattr(self, 'created_audit_log_ids') and self.created_audit_log_ids:
            log_id = self.created_audit_log_ids[0]
            success_owner_verify, response_owner_verify = self.run_test(
                "Audit Verify - Owner Access", 
                "GET", 
                f"audit/verify/{log_id}?judge_id=owner-001", 
                200
            )
            
            if success_owner_verify:
                print(f"   âœ… Owner access to verify granted")
            else:
                all_success = False
            
            # Test 6: Non-owner access to verify endpoint (should return 403)
            success_non_owner_verify, response_non_owner_verify = self.run_test(
                "Audit Verify - Non-Owner Access", 
                "GET", 
                f"audit/verify/{log_id}?judge_id=JUDGE001", 
                403
            )
            
            if success_non_owner_verify:
                print(f"   âœ… Non-owner access to verify correctly denied (403)")
            else:
                all_success = False
        
        # Test 7: Owner access to export endpoint
        success_owner_export, response_owner_export = self.run_test(
            "Audit Export - Owner Access", 
            "GET", 
            "audit/export?judge_id=owner-001", 
            200
        )
        
        if success_owner_export:
            print(f"   âœ… Owner access to export granted")
        else:
            all_success = False
        
        # Test 8: Non-owner access to export endpoint (should return 403)
        success_non_owner_export, response_non_owner_export = self.run_test(
            "Audit Export - Non-Owner Access", 
            "GET", 
            "audit/export?judge_id=JUDGE001", 
            403
        )
        
        if success_non_owner_export:
            print(f"   âœ… Non-owner access to export correctly denied (403)")
        else:
            all_success = False
        
        return all_success

    def test_judge_profile_integration_flow(self):
        """Test complete Judge Profile Management integration flow"""
        print("\nðŸ‘¨â€âš–ï¸ Testing Complete Judge Profile Management Integration Flow...")
        
        # Step 1: Create judge profiles
        print("   Step 1: Creating judge profiles...")
        if not self.test_judge_profile_create():
            return False
        
        # Step 2: Get profiles with stats
        print("   Step 2: Retrieving profiles with calculated stats...")
        if not self.test_judge_profile_get_with_stats():
            return False
        
        # Step 3: Update profile information
        print("   Step 3: Updating profile information...")
        if not self.test_judge_profile_update():
            return False
        
        # Step 4: Get scoring history
        print("   Step 4: Retrieving scoring history...")
        if not self.test_judge_profile_history():
            return False
        
        # Step 5: Test owner access control
        print("   Step 5: Testing owner-only audit access...")
        if not self.test_audit_owner_access_control():
            return False
        
        print("   ðŸŽ‰ Complete Judge Profile Management integration flow test passed!")
        print("   âœ… All 4 judge profile APIs working correctly")
        print("   âœ… Stats calculated from shadow judging submissions")
        print("   âœ… Owner-restricted audit log access working")
        return True

    def test_fighter_stats_create(self):
        """Test creating/updating fighter statistics"""
        print("\nðŸ¥Š Testing Fighter Stats - Create/Update Fighter Stats...")
        
        # Test data for fighter stats updates
        fighter_updates = [
            {
                "fighter_name": "Jon Jones",
                "round_events": [
                    {"event_type": "KD", "metadata": {"severity": "hard"}},
                    {"event_type": "SS Head", "metadata": {"power_strike": True}},
                    {"event_type": "SS Head", "metadata": {}},
                    {"event_type": "Takedown", "metadata": {"immediate_pass": True}},
                    {"event_type": "Pass", "metadata": {}},
                    {"event_type": "CTRL_START", "metadata": {}},
                    {"event_type": "CTRL_STOP", "metadata": {"position": "mount", "effective_control": True}}
                ],
                "round_score": 8.5,
                "round_result": "won",
                "control_time": 120.0,
                "round_card": "10-8"
            },
            {
                "fighter_name": "Daniel Cormier",
                "round_events": [
                    {"event_type": "SS Body", "metadata": {}},
                    {"event_type": "SS Leg", "metadata": {}},
                    {"event_type": "Takedown", "metadata": {}},
                    {"event_type": "Submission Attempt", "metadata": {"depth": "tight", "duration": 15}}
                ],
                "round_score": 6.2,
                "round_result": "lost",
                "control_time": 45.0,
                "round_card": "8-10"
            },
            {
                "fighter_name": "Amanda Nunes",
                "round_events": [
                    {"event_type": "KD", "metadata": {"severity": "near-finish"}},
                    {"event_type": "SS Head", "metadata": {"power_strike": True, "rocked": True}},
                    {"event_type": "SS Head", "metadata": {"power_strike": True}},
                    {"event_type": "SS Body", "metadata": {}}
                ],
                "round_score": 9.1,
                "round_result": "won",
                "control_time": 0.0,
                "round_card": "10-9"
            }
        ]
        
        all_success = True
        created_fighters = []
        
        for i, update_data in enumerate(fighter_updates):
            success, response = self.run_test(
                f"Update Fighter Stats - {update_data['fighter_name']}", 
                "POST", 
                "fighters/update-stats", 
                200, 
                update_data
            )
            
            if success and response:
                print(f"   âœ… Fighter stats updated: {update_data['fighter_name']}")
                
                # Verify response structure
                required_fields = ['success', 'message']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    created_fighters.append(update_data['fighter_name'])
                    print(f"   Message: {response['message']}")
            else:
                all_success = False
        
        # Store created fighter names for later tests
        self.created_fighters = created_fighters
        return all_success

    def test_fighter_stats_get(self):
        """Test retrieving fighter statistics"""
        print("\nðŸ“Š Testing Fighter Stats - Get Fighter Statistics...")
        
        if not hasattr(self, 'created_fighters') or not self.created_fighters:
            print("   âŒ No fighter names available from previous test")
            return False
        
        all_success = True
        
        # Test getting each created fighter's stats
        import urllib.parse
        for fighter_name in self.created_fighters:
            encoded_name = urllib.parse.quote(fighter_name, safe='')
            success, response = self.run_test(
                f"Get Fighter Stats - {fighter_name}", 
                "GET", 
                f"fighters/{encoded_name}/stats", 
                200
            )
            
            if success and response:
                print(f"   âœ… Stats retrieved for fighter: {fighter_name}")
                
                # Verify response structure
                required_fields = ['fighter_name', 'total_rounds', 'total_fights', 'avg_kd_per_round', 
                                 'avg_ss_per_round', 'avg_td_per_round', 'avg_sub_attempts', 'avg_passes', 
                                 'avg_reversals', 'avg_control_time', 'avg_round_score', 'rounds_won', 
                                 'rounds_lost', 'rounds_drawn', 'rate_10_8', 'rate_10_7', 'tendencies', 'last_updated']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    print(f"   Total Rounds: {response['total_rounds']}")
                    print(f"   Avg KD per Round: {response['avg_kd_per_round']}")
                    print(f"   Avg SS per Round: {response['avg_ss_per_round']}")
                    print(f"   Rounds Won: {response['rounds_won']}")
                    
                    # Verify tendencies structure
                    tendencies = response.get('tendencies', {})
                    if tendencies:
                        required_tendency_fields = ['striking_style', 'grappling_rate', 'finish_threat_rate', 
                                                  'control_preference', 'aggression_level']
                        missing_tendency_fields = [field for field in required_tendency_fields if field not in tendencies]
                        
                        if missing_tendency_fields:
                            print(f"   âš ï¸  Missing fields in tendencies: {missing_tendency_fields}")
                            all_success = False
                        else:
                            print(f"   Grappling Rate: {tendencies['grappling_rate']}")
                            print(f"   Aggression Level: {tendencies['aggression_level']}")
            else:
                all_success = False
        
        # Test 404 for non-existent fighter
        success_404, _ = self.run_test("Get Fighter Stats - Non-existent", "GET", "fighters/NonExistentFighter/stats", 404)
        
        return all_success and success_404

    def test_discrepancy_flags_create(self):
        """Test creating discrepancy flags"""
        print("\nðŸš© Testing Discrepancy Flags - Create Flags...")
        
        # Test data for different types of flags
        flag_data = [
            {
                "bout_id": "UFC_299_001",
                "round_num": 1,
                "flag_type": "boundary_case",
                "severity": "medium",
                "description": "Score very close to 10-8 threshold",
                "context": {
                    "delta": 595,
                    "threshold": 600,
                    "card": "10-9"
                }
            },
            {
                "bout_id": "UFC_299_002",
                "round_num": 2,
                "flag_type": "tie_breaker",
                "severity": "high",
                "description": "Round decided by damage tie-breaker",
                "context": {
                    "tie_breaker": "damage",
                    "delta": 1.0,
                    "card": "10-9"
                }
            },
            {
                "bout_id": "UFC_299_003",
                "round_num": 3,
                "flag_type": "low_activity",
                "severity": "low",
                "description": "Very low activity round",
                "context": {
                    "event_count": 3,
                    "card": "10-9"
                }
            },
            {
                "bout_id": "UFC_299_004",
                "round_num": 1,
                "flag_type": "statistical_anomaly",
                "severity": "high",
                "description": "10-8 without standard dominance gates",
                "context": {
                    "card": "10-8",
                    "gates": {
                        "finish_threat": False,
                        "control_dom": False,
                        "multi_cat_dom": False
                    }
                }
            }
        ]
        
        all_success = True
        created_flag_ids = []
        
        for i, flag in enumerate(flag_data):
            success, response = self.run_test(
                f"Create Flag - {flag['flag_type']}", 
                "POST", 
                "review/create-flag", 
                200, 
                flag
            )
            
            if success and response:
                print(f"   âœ… Flag created: {flag['flag_type']}")
                
                # Verify response structure
                required_fields = ['success', 'flag_id']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    created_flag_ids.append(response['flag_id'])
                    print(f"   Flag ID: {response['flag_id']}")
            else:
                all_success = False
        
        # Store created flag IDs for later tests
        self.created_flag_ids = created_flag_ids
        return all_success

    def test_discrepancy_flags_get_all(self):
        """Test retrieving all discrepancy flags with filters"""
        print("\nðŸ“‹ Testing Discrepancy Flags - Get All Flags...")
        
        # Test 1: Get all flags (no filters)
        success1, response1 = self.run_test("Get All Flags", "GET", "review/flags", 200)
        
        if success1 and response1:
            flags = response1.get('flags', [])
            count = response1.get('count', 0)
            
            print(f"   âœ… Retrieved {count} flags")
            
            # Verify response structure
            required_fields = ['flags', 'count']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify flag structure if flags exist
            if flags:
                first_flag = flags[0]
                required_flag_fields = ['id', 'bout_id', 'round_num', 'flag_type', 'severity', 
                                      'description', 'context', 'status', 'created_at']
                missing_flag_fields = [field for field in required_flag_fields if field not in first_flag]
                
                if missing_flag_fields:
                    print(f"   âš ï¸  Missing fields in flag structure: {missing_flag_fields}")
                    return False
                
                print(f"   âœ… Flag structure validated")
                print(f"   Sample flag: {first_flag['flag_type']} - {first_flag['severity']} severity")
        
        # Test 2: Filter by status
        success2, response2 = self.run_test("Get Flags - Filter by status", "GET", "review/flags?status=pending", 200)
        
        if success2 and response2:
            filtered_flags = response2.get('flags', [])
            print(f"   âœ… Filtered by status: {len(filtered_flags)} flags")
            
            # Verify all flags have the correct status
            for flag in filtered_flags:
                if flag.get('status') != 'pending':
                    print(f"   âš ï¸  Filter failed: found flag with status '{flag.get('status')}'")
                    return False
        
        # Test 3: Filter by severity
        success3, response3 = self.run_test("Get Flags - Filter by severity", "GET", "review/flags?severity=high", 200)
        
        if success3 and response3:
            severity_flags = response3.get('flags', [])
            print(f"   âœ… Filtered by severity: {len(severity_flags)} flags")
        
        # Test 4: Filter by flag_type
        success4, response4 = self.run_test("Get Flags - Filter by flag_type", "GET", "review/flags?flag_type=boundary_case", 200)
        
        if success4 and response4:
            type_flags = response4.get('flags', [])
            print(f"   âœ… Filtered by flag_type: {len(type_flags)} flags")
        
        return success1 and success2 and success3 and success4

    def test_discrepancy_flags_get_by_bout(self):
        """Test retrieving flags for specific bout"""
        print("\nðŸŽ¯ Testing Discrepancy Flags - Get Flags by Bout...")
        
        # Test getting flags for a specific bout
        success, response = self.run_test("Get Bout Flags", "GET", "review/flags/UFC_299_001", 200)
        
        if success and response:
            flags = response.get('flags', [])
            count = response.get('count', 0)
            
            print(f"   âœ… Retrieved {count} flags for bout UFC_299_001")
            
            # Verify response structure
            required_fields = ['flags', 'count']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify all flags belong to the correct bout
            for flag in flags:
                if flag.get('bout_id') != 'UFC_299_001':
                    print(f"   âš ï¸  Found flag for wrong bout: {flag.get('bout_id')}")
                    return False
            
            # Verify flags are sorted by round_num
            if len(flags) > 1:
                for i in range(len(flags) - 1):
                    current_round = flags[i]['round_num']
                    next_round = flags[i + 1]['round_num']
                    if current_round > next_round:
                        print(f"   âš ï¸  Flags not sorted by round number")
                        return False
                
                print(f"   âœ… Flags properly sorted by round number")
        
        return success

    def test_tuning_profiles_create(self):
        """Test creating tuning profiles"""
        print("\nâš™ï¸ Testing Tuning Profiles - Create Profiles...")
        
        # Test data for different tuning profiles
        profile_data = [
            {
                "name": "UFC Standard",
                "promotion": "UFC",
                "description": "Standard UFC judging criteria with emphasis on damage",
                "weights": {
                    "KD": 0.35,
                    "ISS": 0.25,
                    "TSR": 0.15,
                    "GCQ": 0.08,
                    "TDQ": 0.07,
                    "OC": 0.05,
                    "SUBQ": 0.03,
                    "AGG": 0.02,
                    "RP": 0.00
                },
                "thresholds": {
                    "threshold_10_9": 550,
                    "threshold_10_8": 850
                },
                "gate_sensitivity": {
                    "finish_threat_kd_threshold": 0.8,
                    "finish_threat_subq_threshold": 7.5,
                    "finish_threat_iss_threshold": 8.5,
                    "control_dom_gcq_threshold": 7.0,
                    "control_dom_time_share": 0.6,
                    "multi_cat_dom_count": 3,
                    "multi_cat_dom_score_threshold": 7.0
                },
                "created_by": "admin-001"
            },
            {
                "name": "Bellator Aggressive",
                "promotion": "Bellator",
                "description": "Bellator style with higher aggression weighting",
                "weights": {
                    "KD": 0.30,
                    "ISS": 0.20,
                    "TSR": 0.18,
                    "GCQ": 0.10,
                    "TDQ": 0.08,
                    "OC": 0.06,
                    "SUBQ": 0.05,
                    "AGG": 0.03,
                    "RP": 0.00
                },
                "created_by": "bellator-admin"
            },
            {
                "name": "ONE Championship",
                "promotion": "ONE Championship",
                "description": "ONE FC criteria emphasizing technical skill",
                "weights": {
                    "KD": 0.25,
                    "ISS": 0.22,
                    "TSR": 0.15,
                    "GCQ": 0.12,
                    "TDQ": 0.10,
                    "OC": 0.08,
                    "SUBQ": 0.06,
                    "AGG": 0.02,
                    "RP": 0.00
                },
                "created_by": "one-admin"
            }
        ]
        
        all_success = True
        created_profile_ids = []
        
        for i, profile in enumerate(profile_data):
            success, response = self.run_test(
                f"Create Tuning Profile - {profile['name']}", 
                "POST", 
                "tuning-profiles/create", 
                200, 
                profile
            )
            
            if success and response:
                print(f"   âœ… Tuning profile created: {profile['name']}")
                
                # Verify response structure
                required_fields = ['success', 'profile_id', 'profile']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    created_profile_ids.append(response['profile_id'])
                    print(f"   Profile ID: {response['profile_id']}")
                    profile_obj = response.get('profile', {})
                    print(f"   Promotion: {profile_obj.get('promotion', 'N/A')}")
            else:
                all_success = False
        
        # Store created profile IDs for later tests
        self.created_profile_ids = created_profile_ids
        return all_success

    def test_tuning_profiles_get_all(self):
        """Test retrieving all tuning profiles"""
        print("\nðŸ“‹ Testing Tuning Profiles - Get All Profiles...")
        
        success, response = self.run_test("Get All Tuning Profiles", "GET", "tuning-profiles", 200)
        
        if success and response:
            profiles = response.get('profiles', [])
            count = response.get('count', 0)
            print(f"   âœ… Retrieved {count} tuning profiles")
            
            # Verify profile structure if profiles exist
            if profiles:
                first_profile = profiles[0]
                required_fields = ['id', 'name', 'promotion', 'description', 'weights', 'thresholds', 
                                 'gate_sensitivity', 'is_default', 'created_by', 'created_at', 'updated_at']
                missing_fields = [field for field in required_fields if field not in first_profile]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in profile structure: {missing_fields}")
                    return False
                
                print(f"   âœ… Profile structure validated")
                print(f"   Sample profile: {first_profile['name']} ({first_profile['promotion']})")
                
                # Verify weights structure
                weights = first_profile.get('weights', {})
                expected_weight_keys = ['KD', 'ISS', 'TSR', 'GCQ', 'TDQ', 'OC', 'SUBQ', 'AGG', 'RP']
                missing_weight_keys = [key for key in expected_weight_keys if key not in weights]
                
                if missing_weight_keys:
                    print(f"   âš ï¸  Missing weight keys: {missing_weight_keys}")
                    return False
                
                # Verify thresholds structure
                thresholds = first_profile.get('thresholds', {})
                expected_threshold_keys = ['threshold_10_9', 'threshold_10_8']
                missing_threshold_keys = [key for key in expected_threshold_keys if key not in thresholds]
                
                if missing_threshold_keys:
                    print(f"   âš ï¸  Missing threshold keys: {missing_threshold_keys}")
                    return False
                
                print(f"   âœ… Weights and thresholds structure validated")
        
        return success

    def test_tuning_profiles_get_by_id(self):
        """Test retrieving specific tuning profile by ID"""
        print("\nðŸŽ¯ Testing Tuning Profiles - Get Profile by ID...")
        
        if not hasattr(self, 'created_profile_ids') or not self.created_profile_ids:
            print("   âŒ No profile IDs available from previous test")
            return False
        
        all_success = True
        
        # Test getting each created profile
        for profile_id in self.created_profile_ids:
            success, response = self.run_test(
                f"Get Tuning Profile - {profile_id}", 
                "GET", 
                f"tuning-profiles/{profile_id}", 
                200
            )
            
            if success and response:
                print(f"   âœ… Profile retrieved: {profile_id}")
                
                # Verify response structure
                required_fields = ['id', 'name', 'promotion', 'weights', 'thresholds', 'gate_sensitivity']
                missing_fields = [field for field in required_fields if field not in response]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                    all_success = False
                else:
                    print(f"   Name: {response['name']}")
                    print(f"   Promotion: {response['promotion']}")
            else:
                all_success = False
        
        # Test 404 for non-existent profile
        success_404, _ = self.run_test("Get Tuning Profile - Non-existent", "GET", "tuning-profiles/non-existent-id", 404)
        
        return all_success and success_404

    def test_tuning_profiles_update(self):
        """Test updating tuning profiles"""
        print("\nâœï¸ Testing Tuning Profiles - Update Profiles...")
        
        if not hasattr(self, 'created_profile_ids') or not self.created_profile_ids:
            print("   âŒ No profile IDs available from previous test")
            return False
        
        # Test updating the first profile
        profile_id = self.created_profile_ids[0]
        
        # Update data
        update_data = {
            "name": "UFC Standard v2.0",
            "description": "Updated UFC judging criteria with refined damage weighting",
            "weights": {
                "KD": 0.40,
                "ISS": 0.25,
                "TSR": 0.15,
                "GCQ": 0.08,
                "TDQ": 0.07,
                "OC": 0.03,
                "SUBQ": 0.02,
                "AGG": 0.00,
                "RP": 0.00
            }
        }
        
        success, response = self.run_test(
            f"Update Tuning Profile - {profile_id}", 
            "PUT", 
            f"tuning-profiles/{profile_id}", 
            200, 
            update_data
        )
        
        if success and response:
            print(f"   âœ… Tuning profile updated: {profile_id}")
            
            # Verify response structure
            required_fields = ['id', 'name', 'description', 'weights', 'updated_at']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
            
            # Verify updates were applied
            if response['name'] != update_data['name']:
                print(f"   âš ï¸  Name not updated correctly")
                return False
            
            if response['description'] != update_data['description']:
                print(f"   âš ï¸  Description not updated correctly")
                return False
            
            # Verify weights were updated
            response_weights = response.get('weights', {})
            for key, value in update_data['weights'].items():
                if response_weights.get(key) != value:
                    print(f"   âš ï¸  Weight {key} not updated correctly")
                    return False
            
            print(f"   Updated Name: {response['name']}")
            print(f"   Updated KD Weight: {response_weights.get('KD', 'N/A')}")
        
        # Test 404 for non-existent profile
        success_404, _ = self.run_test("Update Tuning Profile - Non-existent", "PUT", "tuning-profiles/non-existent-id", 404, update_data)
        
        return success and success_404

    def test_tuning_profiles_delete(self):
        """Test deleting tuning profiles"""
        print("\nðŸ—‘ï¸ Testing Tuning Profiles - Delete Profiles...")
        
        if not hasattr(self, 'created_profile_ids') or len(self.created_profile_ids) < 2:
            print("   âŒ Need at least 2 profile IDs for delete test")
            return False
        
        # Delete the last created profile (keep others for integration tests)
        profile_id = self.created_profile_ids[-1]
        
        success, response = self.run_test(
            f"Delete Tuning Profile - {profile_id}", 
            "DELETE", 
            f"tuning-profiles/{profile_id}", 
            200
        )
        
        if success and response:
            print(f"   âœ… Tuning profile deleted: {profile_id}")
            
            # Verify response structure
            if 'success' not in response or 'message' not in response:
                print(f"   âš ï¸  Missing fields in response: expected 'success' and 'message'")
                return False
            
            print(f"   Message: {response['message']}")
            
            # Verify profile is actually deleted by trying to get it
            success_verify, _ = self.run_test(
                f"Verify Deletion - {profile_id}", 
                "GET", 
                f"tuning-profiles/{profile_id}", 
                404
            )
            
            if success_verify:
                print(f"   âœ… Profile deletion verified (404 on GET)")
            else:
                print(f"   âš ï¸  Profile still exists after deletion")
                return False
        
        # Test 404 for non-existent profile
        success_404, _ = self.run_test("Delete Tuning Profile - Non-existent", "DELETE", "tuning-profiles/non-existent-id", 404)
        
        return success and success_404

    def test_fighter_stats_integration_flow(self):
        """Test complete Fighter Stats integration flow"""
        print("\nðŸ¥Š Testing Complete Fighter Stats Integration Flow...")
        
        # Step 1: Create/update fighter stats
        print("   Step 1: Creating/updating fighter statistics...")
        if not self.test_fighter_stats_create():
            return False
        
        # Step 2: Retrieve fighter stats
        print("   Step 2: Retrieving fighter statistics...")
        if not self.test_fighter_stats_get():
            return False
        
        print("   ðŸŽ‰ Complete Fighter Stats integration flow test passed!")
        print("   âœ… Fighter stats creation and retrieval working correctly")
        print("   âœ… Tendencies calculation and storage verified")
        return True

    def test_discrepancy_flags_integration_flow(self):
        """Test complete Discrepancy Flags integration flow"""
        print("\nðŸš© Testing Complete Discrepancy Flags Integration Flow...")
        
        # Step 1: Create flags
        print("   Step 1: Creating discrepancy flags...")
        if not self.test_discrepancy_flags_create():
            return False
        
        # Step 2: Get all flags with filters
        print("   Step 2: Retrieving flags with filters...")
        if not self.test_discrepancy_flags_get_all():
            return False
        
        # Step 3: Get flags by bout
        print("   Step 3: Retrieving flags by bout...")
        if not self.test_discrepancy_flags_get_by_bout():
            return False
        
        print("   ðŸŽ‰ Complete Discrepancy Flags integration flow test passed!")
        print("   âœ… All flag creation and retrieval APIs working correctly")
        print("   âœ… Filtering and bout-specific queries verified")
        return True

    def test_tuning_profiles_integration_flow(self):
        """Test complete Tuning Profiles integration flow"""
        print("\nâš™ï¸ Testing Complete Tuning Profiles Integration Flow...")
        
        # Step 1: Create tuning profiles
        print("   Step 1: Creating tuning profiles...")
        if not self.test_tuning_profiles_create():
            return False
        
        # Step 2: Get all profiles
        print("   Step 2: Retrieving all tuning profiles...")
        if not self.test_tuning_profiles_get_all():
            return False
        
        # Step 3: Get profiles by ID
        print("   Step 3: Retrieving profiles by ID...")
        if not self.test_tuning_profiles_get_by_id():
            return False
        
        # Step 4: Update profiles
        print("   Step 4: Updating tuning profiles...")
        if not self.test_tuning_profiles_update():
            return False
        
        # Step 5: Delete profiles
        print("   Step 5: Deleting tuning profiles...")
        if not self.test_tuning_profiles_delete():
            return False
        
        print("   ðŸŽ‰ Complete Tuning Profiles integration flow test passed!")
        print("   âœ… All 5 tuning profile APIs working correctly")
        print("   âœ… CRUD operations and data validation verified")
        return True

    def run_judge_profile_tests_only(self):
        """Run only Judge Profile Management tests"""
        print("ðŸš€ Starting Judge Profile Management API Tests")
        print(f"Testing against: {self.base_url}")
        print("=" * 60)
        
        # Test Judge Profile Management Feature
        self.test_judge_profile_integration_flow()
        
        # Print summary
        print("\n" + "=" * 60)
        print(f"ðŸ“Š Test Summary: {self.tests_passed}/{self.tests_run} tests passed")
        
        if self.tests_passed == self.tests_run:
            print("ðŸŽ‰ All Judge Profile Management tests passed!")
            return 0
        else:
            print("âŒ Some Judge Profile Management tests failed")
            return 1

    def test_control_timer_basic_start_stop(self):
        """Test Scenario 1: Basic Start/Stop Cycle for control timer events"""
        print("\nâ±ï¸ Testing Control Timer - Basic Start/Stop Cycle...")
        
        # Create test events for Ground Top Control timer
        test_events = [
            # Start event
            {
                "bout_id": "test_control_timer_001",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 30.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 30.0
                }
            },
            # Stop event after 10 seconds
            {
                "bout_id": "test_control_timer_001", 
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 40.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 10.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_001",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Control Timer Basic Start/Stop", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Control timer events processed successfully")
            
            # Verify fighter1 has control time events in event_counts
            fighter1_score = response.get('fighter1_score', {})
            fighter1_event_counts = fighter1_score.get('event_counts', {})
            
            # Check if Ground Top Control events are counted
            ground_top_control_count = fighter1_event_counts.get('Ground Top Control', 0)
            if ground_top_control_count != 2:  # Should have start + stop events
                print(f"   âš ï¸  Expected 2 Ground Top Control events, got {ground_top_control_count}")
                return False
            
            print(f"   âœ… Ground Top Control events counted: {ground_top_control_count}")
            
            # Verify scoring includes duration-based calculation
            fighter1_final_score = fighter1_score.get('final_score', 0)
            if fighter1_final_score <= 0:
                print(f"   âš ï¸  Fighter1 should have positive score from control time")
                return False
            
            print(f"   âœ… Fighter1 score includes control time: {fighter1_final_score}")
            
            # Verify metadata structure is preserved
            print(f"   âœ… Control timer metadata structure validated")
            
        return success

    def test_control_timer_resume_from_pause(self):
        """Test Scenario 2: Resume from Paused State"""
        print("\nâ¸ï¸ Testing Control Timer - Resume from Paused State...")
        
        # Create test events for Ground Back Control with pause/resume
        test_events = [
            # First control period - 5 seconds
            {
                "bout_id": "test_control_timer_002",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Ground Back Control",
                "timestamp": 20.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 20.0
                }
            },
            {
                "bout_id": "test_control_timer_002",
                "round_num": 1,
                "fighter": "fighter2", 
                "event_type": "Ground Back Control",
                "timestamp": 25.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 5.0
                }
            },
            # Second control period - another 5 seconds (total 10)
            {
                "bout_id": "test_control_timer_002",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Ground Back Control", 
                "timestamp": 50.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 50.0
                }
            },
            {
                "bout_id": "test_control_timer_002",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Ground Back Control",
                "timestamp": 55.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 5.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_002",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Control Timer Resume from Pause", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Resume control timer events processed successfully")
            
            # Verify fighter2 has accumulated control time
            fighter2_score = response.get('fighter2_score', {})
            fighter2_event_counts = fighter2_score.get('event_counts', {})
            
            # Check Ground Back Control events
            ground_back_control_count = fighter2_event_counts.get('Ground Back Control', 0)
            if ground_back_control_count != 4:  # Should have 2 start + 2 stop events
                print(f"   âš ï¸  Expected 4 Ground Back Control events, got {ground_back_control_count}")
                return False
            
            print(f"   âœ… Ground Back Control events counted: {ground_back_control_count}")
            
            # Verify accumulated scoring (should reflect total 10 seconds of control)
            fighter2_final_score = fighter2_score.get('final_score', 0)
            if fighter2_final_score <= 0:
                print(f"   âš ï¸  Fighter2 should have positive score from accumulated control time")
                return False
            
            print(f"   âœ… Fighter2 accumulated control score: {fighter2_final_score}")
            
        return success

    def test_control_timer_switch_types(self):
        """Test Scenario 3: Switch Between Control Types"""
        print("\nðŸ”„ Testing Control Timer - Switch Between Control Types...")
        
        # Create test events switching from Ground Top Control to Ground Back Control
        test_events = [
            # Ground Top Control for 8 seconds
            {
                "bout_id": "test_control_timer_003",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 10.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 10.0
                }
            },
            {
                "bout_id": "test_control_timer_003",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 18.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 8.0
                }
            },
            # Switch to Ground Back Control for 5 seconds
            {
                "bout_id": "test_control_timer_003",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Back Control",
                "timestamp": 20.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 20.0
                }
            },
            {
                "bout_id": "test_control_timer_003",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Back Control",
                "timestamp": 25.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 5.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_003",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Control Timer Switch Types", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Control type switching events processed successfully")
            
            # Verify fighter1 has both control types
            fighter1_score = response.get('fighter1_score', {})
            fighter1_event_counts = fighter1_score.get('event_counts', {})
            
            # Check both control types are counted
            ground_top_count = fighter1_event_counts.get('Ground Top Control', 0)
            ground_back_count = fighter1_event_counts.get('Ground Back Control', 0)
            
            if ground_top_count != 2:  # Should have 1 start + 1 stop
                print(f"   âš ï¸  Expected 2 Ground Top Control events, got {ground_top_count}")
                return False
                
            if ground_back_count != 2:  # Should have 1 start + 1 stop
                print(f"   âš ï¸  Expected 2 Ground Back Control events, got {ground_back_count}")
                return False
            
            print(f"   âœ… Ground Top Control events: {ground_top_count}")
            print(f"   âœ… Ground Back Control events: {ground_back_count}")
            
            # Verify proper event sequence and scoring
            fighter1_final_score = fighter1_score.get('final_score', 0)
            if fighter1_final_score <= 0:
                print(f"   âš ï¸  Fighter1 should have positive score from mixed control types")
                return False
            
            print(f"   âœ… Fighter1 mixed control score: {fighter1_final_score}")
            
        return success

    def test_control_timer_backend_scoring_integration(self):
        """Test Scenario 4: Backend Scoring Integration"""
        print("\nðŸŽ¯ Testing Control Timer - Backend Scoring Integration...")
        
        # Create test events with 30 seconds of Ground Top Control
        test_events = [
            {
                "bout_id": "test_control_timer_004",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 60.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 60.0
                }
            },
            {
                "bout_id": "test_control_timer_004",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 90.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 30.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_004",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Control Timer Backend Integration", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Backend scoring integration successful")
            
            # Verify the backend correctly processes duration and applies value_per_sec scoring
            fighter1_score = response.get('fighter1_score', {})
            fighter1_final_score = fighter1_score.get('final_score', 0)
            
            # Based on SCORING_CONFIG: Ground Top Control has value_per_sec: 0.010
            # 30 seconds * 0.010 = 0.30 base value
            # Then normalized and weighted: 0.30 * 100 * 0.40 (grappling category) = 12.0
            expected_min_score = 10.0  # Should be at least this much from 30 seconds of control
            
            if fighter1_final_score < expected_min_score:
                print(f"   âš ï¸  Expected score >= {expected_min_score} for 30s control, got {fighter1_final_score}")
                return False
            
            print(f"   âœ… Control time properly scored: {fighter1_final_score}")
            
            # Verify event counts
            fighter1_event_counts = fighter1_score.get('event_counts', {})
            ground_top_count = fighter1_event_counts.get('Ground Top Control', 0)
            
            if ground_top_count != 2:
                print(f"   âš ï¸  Expected 2 Ground Top Control events, got {ground_top_count}")
                return False
            
            print(f"   âœ… Event counts correct: {ground_top_count} events")
            
            # Verify score is proportional to duration
            print(f"   âœ… Duration-based scoring verified for 30 seconds")
            
        return success

    def test_control_timer_cage_control(self):
        """Test Cage Control Time events"""
        print("\nðŸŸï¸ Testing Control Timer - Cage Control Time...")
        
        # Create test events for Cage Control Time
        test_events = [
            {
                "bout_id": "test_control_timer_005",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Cage Control Time",
                "timestamp": 45.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 45.0
                }
            },
            {
                "bout_id": "test_control_timer_005",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Cage Control Time",
                "timestamp": 65.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 20.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_005",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Cage Control Time Events", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Cage Control Time events processed successfully")
            
            # Verify fighter2 has cage control events
            fighter2_score = response.get('fighter2_score', {})
            fighter2_event_counts = fighter2_score.get('event_counts', {})
            
            cage_control_count = fighter2_event_counts.get('Cage Control Time', 0)
            if cage_control_count != 2:
                print(f"   âš ï¸  Expected 2 Cage Control Time events, got {cage_control_count}")
                return False
            
            print(f"   âœ… Cage Control Time events counted: {cage_control_count}")
            
            # Verify scoring (Cage Control is in "other" category with value_per_sec: 0.006)
            fighter2_final_score = fighter2_score.get('final_score', 0)
            if fighter2_final_score <= 0:
                print(f"   âš ï¸  Fighter2 should have positive score from cage control")
                return False
            
            print(f"   âœ… Cage control properly scored: {fighter2_final_score}")
            
        return success

    def test_control_timer_mixed_scenario(self):
        """Test mixed control timer scenario with multiple fighters and types"""
        print("\nðŸ¥Š Testing Control Timer - Mixed Scenario...")
        
        # Create complex test scenario with multiple control types and fighters
        test_events = [
            # Fighter 1: Ground Top Control
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 30.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 30.0
                }
            },
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Ground Top Control",
                "timestamp": 45.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 15.0
                }
            },
            # Fighter 2: Ground Back Control
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Ground Back Control",
                "timestamp": 60.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 60.0
                }
            },
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter2",
                "event_type": "Ground Back Control",
                "timestamp": 80.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 20.0
                }
            },
            # Fighter 1: Cage Control
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Cage Control Time",
                "timestamp": 100.0,
                "metadata": {
                    "type": "start",
                    "source": "control-timer",
                    "startTime": 100.0
                }
            },
            {
                "bout_id": "test_control_timer_006",
                "round_num": 1,
                "fighter": "fighter1",
                "event_type": "Cage Control Time",
                "timestamp": 110.0,
                "metadata": {
                    "type": "stop",
                    "source": "control-timer",
                    "duration": 10.0
                }
            }
        ]
        
        score_request = {
            "bout_id": "test_control_timer_006",
            "round_num": 1,
            "events": test_events,
            "round_duration": 300
        }
        
        success, response = self.run_test("Control Timer Mixed Scenario", "POST", "calculate-score-v2", 200, score_request)
        
        if success and response:
            print(f"   âœ… Mixed control timer scenario processed successfully")
            
            # Verify both fighters have appropriate control events
            fighter1_score = response.get('fighter1_score', {})
            fighter2_score = response.get('fighter2_score', {})
            
            fighter1_counts = fighter1_score.get('event_counts', {})
            fighter2_counts = fighter2_score.get('event_counts', {})
            
            # Fighter 1 should have Ground Top Control and Cage Control
            f1_ground_top = fighter1_counts.get('Ground Top Control', 0)
            f1_cage_control = fighter1_counts.get('Cage Control Time', 0)
            
            if f1_ground_top != 2:
                print(f"   âš ï¸  Fighter1 expected 2 Ground Top Control events, got {f1_ground_top}")
                return False
                
            if f1_cage_control != 2:
                print(f"   âš ï¸  Fighter1 expected 2 Cage Control events, got {f1_cage_control}")
                return False
            
            # Fighter 2 should have Ground Back Control
            f2_ground_back = fighter2_counts.get('Ground Back Control', 0)
            
            if f2_ground_back != 2:
                print(f"   âš ï¸  Fighter2 expected 2 Ground Back Control events, got {f2_ground_back}")
                return False
            
            print(f"   âœ… Fighter1 - Ground Top: {f1_ground_top}, Cage Control: {f1_cage_control}")
            print(f"   âœ… Fighter2 - Ground Back: {f2_ground_back}")
            
            # Verify both fighters have positive scores
            f1_score = fighter1_score.get('final_score', 0)
            f2_score = fighter2_score.get('final_score', 0)
            
            if f1_score <= 0 or f2_score <= 0:
                print(f"   âš ï¸  Both fighters should have positive scores")
                return False
            
            print(f"   âœ… Fighter1 total score: {f1_score}")
            print(f"   âœ… Fighter2 total score: {f2_score}")
            
        return success

    def test_control_timer_complete_flow(self):
        """Test complete control timer event logging flow"""
        print("\nðŸŽ¯ Testing Complete Control Timer Event Logging Flow...")
        
        # Run all control timer test scenarios
        scenarios = [
            ("Basic Start/Stop Cycle", self.test_control_timer_basic_start_stop),
            ("Resume from Paused State", self.test_control_timer_resume_from_pause),
            ("Switch Between Control Types", self.test_control_timer_switch_types),
            ("Backend Scoring Integration", self.test_control_timer_backend_scoring_integration),
            ("Cage Control Time", self.test_control_timer_cage_control),
            ("Mixed Scenario", self.test_control_timer_mixed_scenario)
        ]
        
        all_success = True
        passed_scenarios = 0
        
        for scenario_name, test_method in scenarios:
            print(f"\n   Running: {scenario_name}")
            if test_method():
                passed_scenarios += 1
                print(f"   âœ… {scenario_name} - PASSED")
            else:
                all_success = False
                print(f"   âŒ {scenario_name} - FAILED")
        
        print(f"\nðŸŽ‰ Control Timer Event Logging Flow Results:")
        print(f"   Scenarios Passed: {passed_scenarios}/{len(scenarios)}")
        print(f"   Success Rate: {(passed_scenarios / len(scenarios) * 100):.1f}%")
        
        if all_success:
            print("   âœ… All control timer scenarios working correctly!")
            print("   âœ… Event metadata structure validated")
            print("   âœ… Duration-based scoring verified")
            print("   âœ… Backend integration confirmed")
        else:
            print("   âŒ Some control timer scenarios failed")
        
        return all_success

    def test_round_notes_create(self):
        """Test creating round notes"""
        print("\nðŸ“ Testing Round Notes Engine - Create Notes...")
        
        # Use unique bout ID for this test run
        import time
        unique_bout_id = f"test-bout-{int(time.time())}"
        self.test_bout_id = unique_bout_id
        
        # Test Scenario 1: Create note for Round 1
        note_data_1 = {
            "bout_id": unique_bout_id,
            "round_num": 1,
            "judge_id": "JUDGE001",
            "judge_name": "Test Judge",
            "note_text": "Fighter1 dominated with strikes and ground control",
            "metadata": {"category": "general"}
        }
        
        success1, response1 = self.run_test("Create Round Note - Round 1", "POST", "round-notes", 201, note_data_1)
        
        if success1 and response1:
            print(f"   âœ… Round 1 note created successfully")
            print(f"   Note ID: {response1.get('id', 'N/A')}")
            print(f"   Timestamp: {response1.get('timestamp', 'N/A')}")
            
            # Store note ID for later tests
            self.round1_note_id = response1.get('id')
            
            # Verify response structure
            required_fields = ['id', 'bout_id', 'round_num', 'judge_id', 'judge_name', 'note_text', 'timestamp', 'metadata']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âš ï¸  Missing fields in response: {missing_fields}")
                return False
        else:
            return False
        
        # Test Scenario 2: Create note for Round 2
        note_data_2 = {
            "bout_id": unique_bout_id,
            "round_num": 2,
            "judge_id": "JUDGE001",
            "judge_name": "Test Judge",
            "note_text": "Close round, both fighters traded strikes"
        }
        
        success2, response2 = self.run_test("Create Round Note - Round 2", "POST", "round-notes", 201, note_data_2)
        
        if success2 and response2:
            print(f"   âœ… Round 2 note created successfully")
            self.round2_note_id = response2.get('id')
        else:
            return False
        
        # Test Scenario 3: Create another note for Round 1 (different judge)
        note_data_3 = {
            "bout_id": unique_bout_id,
            "round_num": 1,
            "judge_id": "JUDGE002",
            "judge_name": "Second Judge",
            "note_text": "Agreed - Fighter1 clearly won this round"
        }
        
        success3, response3 = self.run_test("Create Round Note - Round 1 Judge 2", "POST", "round-notes", 201, note_data_3)
        
        if success3 and response3:
            print(f"   âœ… Round 1 note for second judge created successfully")
            self.round1_judge2_note_id = response3.get('id')
        else:
            return False
        
        return success1 and success2 and success3

    def test_round_notes_get_round(self):
        """Test getting notes for specific round"""
        print("\nðŸ“– Testing Round Notes Engine - Get Round Notes...")
        
        # Test Scenario 1: Get all notes for Round 1
        success1, response1 = self.run_test("Get Round 1 Notes", "GET", f"round-notes/{self.test_bout_id}/1", 200)
        
        if success1 and response1:
            notes = response1.get('notes', [])
            count = response1.get('count', 0)
            
            print(f"   âœ… Retrieved {count} notes for Round 1")
            
            # Should have 2 notes for round 1 (from JUDGE001 and JUDGE002)
            if count != 2:
                print(f"   âš ï¸  Expected 2 notes for Round 1, got {count}")
                return False
            
            # Verify notes structure
            for i, note in enumerate(notes):
                required_fields = ['id', 'bout_id', 'round_num', 'judge_id', 'judge_name', 'note_text', 'timestamp']
                missing_fields = [field for field in required_fields if field not in note]
                
                if missing_fields:
                    print(f"   âš ï¸  Missing fields in note {i+1}: {missing_fields}")
                    return False
                
                if note['round_num'] != 1:
                    print(f"   âš ï¸  Note {i+1} has wrong round number: {note['round_num']}")
                    return False
            
            print(f"   âœ… All notes have correct structure and round number")
        else:
            return False
        
        # Test Scenario 2: Get notes for specific judge and round
        success2, response2 = self.run_test("Get Round 1 Notes - Judge Filter", "GET", f"round-notes/{self.test_bout_id}/1?judge_id=JUDGE001", 200)
        
        if success2 and response2:
            notes = response2.get('notes', [])
            count = response2.get('count', 0)
            
            print(f"   âœ… Retrieved {count} notes for Round 1, Judge JUDGE001")
            
            # Should have 1 note for JUDGE001 in round 1
            if count != 1:
                print(f"   âš ï¸  Expected 1 note for JUDGE001 in Round 1, got {count}")
                return False
            
            # Verify it's the correct judge
            if notes[0]['judge_id'] != 'JUDGE001':
                print(f"   âš ï¸  Wrong judge in filtered result: {notes[0]['judge_id']}")
                return False
            
            print(f"   âœ… Judge filter working correctly")
        else:
            return False
        
        return success1 and success2

    def test_round_notes_get_bout(self):
        """Test getting all notes for a bout"""
        print("\nðŸ“š Testing Round Notes Engine - Get Bout Notes...")
        
        # Test Scenario 1: Get all notes for bout
        success1, response1 = self.run_test("Get All Bout Notes", "GET", f"round-notes/{self.test_bout_id}", 200)
        
        if success1 and response1:
            notes = response1.get('notes', [])
            notes_by_round = response1.get('notes_by_round', {})
            total_count = response1.get('total_count', 0)
            
            print(f"   âœ… Retrieved {total_count} total notes for bout")
            
            # Should have 3 notes total (2 for round 1, 1 for round 2)
            if total_count != 3:
                print(f"   âš ï¸  Expected 3 total notes, got {total_count}")
                return False
            
            # Verify notes_by_round structure
            if '1' not in notes_by_round or '2' not in notes_by_round:
                print(f"   âš ï¸  Missing rounds in notes_by_round: {list(notes_by_round.keys())}")
                return False
            
            # Round 1 should have 2 notes, Round 2 should have 1 note
            if len(notes_by_round['1']) != 2:
                print(f"   âš ï¸  Expected 2 notes for Round 1, got {len(notes_by_round['1'])}")
                return False
            
            if len(notes_by_round['2']) != 1:
                print(f"   âš ï¸  Expected 1 note for Round 2, got {len(notes_by_round['2'])}")
                return False
            
            print(f"   âœ… Notes grouped by round correctly")
            print(f"   Round 1: {len(notes_by_round['1'])} notes")
            print(f"   Round 2: {len(notes_by_round['2'])} notes")
        else:
            return False
        
        # Test Scenario 2: Get bout notes for specific judge
        success2, response2 = self.run_test("Get Bout Notes - Judge Filter", "GET", f"round-notes/{self.test_bout_id}?judge_id=JUDGE002", 200)
        
        if success2 and response2:
            notes = response2.get('notes', [])
            total_count = response2.get('total_count', 0)
            
            print(f"   âœ… Retrieved {total_count} notes for JUDGE002")
            
            # Should have 1 note for JUDGE002
            if total_count != 1:
                print(f"   âš ï¸  Expected 1 note for JUDGE002, got {total_count}")
                return False
            
            # Verify it's the correct judge
            if notes[0]['judge_id'] != 'JUDGE002':
                print(f"   âš ï¸  Wrong judge in filtered result: {notes[0]['judge_id']}")
                return False
            
            print(f"   âœ… Judge filter working correctly for bout notes")
        else:
            return False
        
        return success1 and success2

    def test_round_notes_update(self):
        """Test updating round notes"""
        print("\nâœï¸ Testing Round Notes Engine - Update Notes...")
        
        # Use unique bout ID for this test
        import time
        update_bout_id = f"test-bout-update-{int(time.time())}"
        
        # First create a note to update
        note_data = {
            "bout_id": update_bout_id,
            "round_num": 1,
            "judge_id": "UPDATE_TEST",
            "judge_name": "Update Test",
            "note_text": "Original note text"
        }
        
        success_create, response_create = self.run_test("Create Note for Update Test", "POST", "round-notes", 201, note_data)
        
        if not success_create or not response_create:
            print("   âŒ Failed to create note for update test")
            return False
        
        note_id = response_create.get('id')
        print(f"   âœ… Created note for update test: {note_id}")
        
        # Now update the note - need to send as form data
        import urllib.parse
        update_data = urllib.parse.urlencode({"note_text": "Updated note text after review"})
        
        success_update, response_update = self.run_test("Update Round Note", "PUT", f"round-notes/{note_id}", 200, update_data)
        
        if success_update and response_update:
            print(f"   âœ… Note updated successfully")
            
            # Verify response structure
            if 'success' not in response_update or 'message' not in response_update:
                print(f"   âš ï¸  Missing fields in update response")
                return False
            
            if not response_update['success']:
                print(f"   âš ï¸  Update response indicates failure")
                return False
            
            print(f"   Message: {response_update['message']}")
        else:
            return False
        
        # Verify the update worked by getting the note
        success_verify, response_verify = self.run_test("Verify Update", "GET", f"round-notes/{update_bout_id}/1", 200)
        
        if success_verify and response_verify:
            notes = response_verify.get('notes', [])
            
            if not notes:
                print(f"   âš ï¸  No notes found after update")
                return False
            
            updated_note = notes[0]
            if updated_note['note_text'] != "Updated note text after review":
                print(f"   âš ï¸  Note text not updated correctly: {updated_note['note_text']}")
                return False
            
            print(f"   âœ… Update verified: {updated_note['note_text']}")
        else:
            return False
        
        return success_create and success_update and success_verify

    def test_round_notes_delete(self):
        """Test deleting round notes"""
        print("\nðŸ—‘ï¸ Testing Round Notes Engine - Delete Notes...")
        
        # Use unique bout ID for this test
        import time
        delete_bout_id = f"test-bout-delete-{int(time.time())}"
        
        # First create a note to delete
        note_data = {
            "bout_id": delete_bout_id,
            "round_num": 1,
            "judge_id": "DELETE_TEST",
            "judge_name": "Delete Test",
            "note_text": "This note will be deleted"
        }
        
        success_create, response_create = self.run_test("Create Note for Delete Test", "POST", "round-notes", 201, note_data)
        
        if not success_create or not response_create:
            print("   âŒ Failed to create note for delete test")
            return False
        
        note_id = response_create.get('id')
        print(f"   âœ… Created note for delete test: {note_id}")
        
        # Now delete the note
        success_delete, response_delete = self.run_test("Delete Round Note", "DELETE", f"round-notes/{note_id}", 200)
        
        if success_delete and response_delete:
            print(f"   âœ… Note deleted successfully")
            
            # Verify response structure
            if 'success' not in response_delete or 'message' not in response_delete:
                print(f"   âš ï¸  Missing fields in delete response")
                return False
            
            if not response_delete['success']:
                print(f"   âš ï¸  Delete response indicates failure")
                return False
            
            print(f"   Message: {response_delete['message']}")
        else:
            return False
        
        # Verify the deletion worked by getting the notes (should be empty)
        success_verify, response_verify = self.run_test("Verify Deletion", "GET", f"round-notes/{delete_bout_id}/1", 200)
        
        if success_verify and response_verify:
            notes = response_verify.get('notes', [])
            count = response_verify.get('count', 0)
            
            if count != 0:
                print(f"   âš ï¸  Expected 0 notes after deletion, got {count}")
                return False
            
            print(f"   âœ… Deletion verified: {count} notes remaining")
        else:
            return False
        
        return success_create and success_delete and success_verify

    def test_round_notes_error_cases(self):
        """Test error cases for round notes"""
        print("\nâš ï¸ Testing Round Notes Engine - Error Cases...")
        
        # Test 1: Update non-existent note
        import urllib.parse
        update_data = urllib.parse.urlencode({"note_text": "Updated text"})
        success1, _ = self.run_test("Update Non-existent Note", "PUT", "round-notes/fake-note-id-999", 404, update_data)
        
        if success1:
            print(f"   âœ… Update non-existent note returns 404 as expected")
        else:
            return False
        
        # Test 2: Delete non-existent note
        success2, _ = self.run_test("Delete Non-existent Note", "DELETE", "round-notes/fake-note-id-999", 404)
        
        if success2:
            print(f"   âœ… Delete non-existent note returns 404 as expected")
        else:
            return False
        
        # Test 3: Get notes for non-existent bout
        success3, response3 = self.run_test("Get Notes for Non-existent Bout", "GET", "round-notes/non-existent-bout/1", 200)
        
        if success3 and response3:
            notes = response3.get('notes', [])
            count = response3.get('count', 0)
            
            if count != 0:
                print(f"   âš ï¸  Expected 0 notes for non-existent bout, got {count}")
                return False
            
            print(f"   âœ… Non-existent bout returns empty array as expected")
        else:
            return False
        
        return success1 and success2 and success3

    def test_round_notes_complete_flow(self):
        """Test the complete Round Notes Engine flow"""
        print("\nðŸŽ¯ Testing Complete Round Notes Engine Flow...")
        
        # Step 1: Create multiple notes
        print("   Step 1: Creating round notes...")
        if not self.test_round_notes_create():
            return False
        
        # Step 2: Get notes by round
        print("   Step 2: Testing round note retrieval...")
        if not self.test_round_notes_get_round():
            return False
        
        # Step 3: Get notes by bout
        print("   Step 3: Testing bout note retrieval...")
        if not self.test_round_notes_get_bout():
            return False
        
        # Step 4: Update notes
        print("   Step 4: Testing note updates...")
        if not self.test_round_notes_update():
            return False
        
        # Step 5: Delete notes
        print("   Step 5: Testing note deletion...")
        if not self.test_round_notes_delete():
            return False
        
        # Step 6: Test error cases
        print("   Step 6: Testing error cases...")
        if not self.test_round_notes_error_cases():
            return False
        
        print("   ðŸŽ‰ Complete Round Notes Engine flow test passed!")
        print("   âœ… All 5 API endpoints working correctly")
        print("   âœ… Notes stored with proper structure (id, bout_id, round_num, judge_id, judge_name, note_text, timestamp, metadata)")
        print("   âœ… Query filtering by judge_id working")
        print("   âœ… Grouping by round working correctly")
        print("   âœ… Update and delete operations working")
        print("   âœ… Proper error handling for 404 cases")
        print("   âœ… Timestamps automatically generated")
        return True

    def test_supervisor_dashboard(self):
        """Test System 3: Supervisor Dashboard Data Feeds"""
        print("\nðŸŽ¯ Testing System 3: Supervisor Dashboard Data Feeds...")
        
        # Test the dashboard endpoint directly
        bout_id = "test-bout-supervisor-123"
        success, response = self.run_test(
            "Get Supervisor Dashboard",
            "GET",
            f"supervisor/dashboard/{bout_id}",
            200
        )
        
        if success and response:
            print(f"   âœ… Supervisor dashboard retrieved successfully")
            
            # Verify response structure
            required_fields = ['bout_id', 'judge_scores', 'rounds_data', 'total_events', 'total_notes', 'anomalies', 'timestamp']
            missing_fields = [field for field in required_fields if field not in response]
            
            if missing_fields:
                print(f"   âŒ Missing fields in dashboard response: {missing_fields}")
                return False
            
            print(f"   Bout ID: {response['bout_id']}")
            print(f"   Judge Scores Count: {len(response['judge_scores'])}")
            print(f"   Total Events: {response['total_events']}")
            print(f"   Total Notes: {response['total_notes']}")
            print(f"   Anomalies Count: {len(response['anomalies'])}")
            print(f"   Timestamp: {response['timestamp']}")
            
            # Verify rounds_data structure
            rounds_data = response.get('rounds_data', {})
            print(f"   Rounds Data: {rounds_data}")
            
            # Verify anomalies structure
            anomalies = response.get('anomalies', [])
            for anomaly in anomalies:
                if 'round' not in anomaly or 'type' not in anomaly or 'severity' not in anomaly:
                    print(f"   âŒ Invalid anomaly structure: {anomaly}")
                    return False
                print(f"   Anomaly: Round {anomaly['round']}, Type: {anomaly['type']}, Severity: {anomaly['severity']}")
        
        return success

    def test_variance_detection(self):
        """Test System 4: AI Judge Variance Detection"""
        print("\nðŸ” Testing System 4: AI Judge Variance Detection...")
        
        # Test 1: Insufficient judges scenario
        bout_id_insufficient = "test-bout-variance-123"
        success1, response1 = self.run_test(
            "Detect Variance - Insufficient Judges",
            "GET", 
            f"variance/detect/{bout_id_insufficient}/1",
            200
        )
        
        if success1 and response1:
            print(f"   âœ… Insufficient judges test passed")
            
            # Verify response structure
            required_fields = ['bout_id', 'round_num', 'variance_detected', 'message', 'judge_count']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âŒ Missing fields in variance response: {missing_fields}")
                return False
            
            if response1['variance_detected'] != False:
                print(f"   âŒ Expected variance_detected=False for insufficient judges")
                return False
            
            print(f"   Variance Detected: {response1['variance_detected']}")
            print(f"   Message: {response1['message']}")
            print(f"   Judge Count: {response1['judge_count']}")
        
        # Test 2: Variance detection with sufficient judges
        bout_id_variance = "test-bout-variance-456"
        success2, response2 = self.run_test(
            "Detect Variance - With Variance",
            "GET",
            f"variance/detect/{bout_id_variance}/1", 
            200
        )
        
        if success2 and response2:
            print(f"   âœ… Variance detection test completed")
            
            # Verify response structure for variance detection
            required_fields = ['bout_id', 'round_num', 'variance_detected', 'max_variance', 
                             'fighter1_variance', 'fighter2_variance', 'severity', 'outliers', 'judge_count']
            missing_fields = [field for field in required_fields if field not in response2]
            
            if missing_fields:
                print(f"   âŒ Missing fields in variance response: {missing_fields}")
                return False
            
            print(f"   Variance Detected: {response2['variance_detected']}")
            print(f"   Max Variance: {response2['max_variance']}")
            print(f"   Fighter 1 Variance: {response2['fighter1_variance']}")
            print(f"   Fighter 2 Variance: {response2['fighter2_variance']}")
            print(f"   Severity: {response2['severity']}")
            print(f"   Outliers Count: {len(response2['outliers'])}")
            print(f"   Judge Count: {response2['judge_count']}")
            
            # Verify outliers structure
            for outlier in response2['outliers']:
                required_outlier_fields = ['judge_id', 'judge_name', 'card', 'fighter1_score', 'fighter2_score']
                missing_outlier_fields = [field for field in required_outlier_fields if field not in outlier]
                if missing_outlier_fields:
                    print(f"   âŒ Missing fields in outlier: {missing_outlier_fields}")
                    return False
        
        return success1 and success2

    def test_promotion_branding(self):
        """Test System 6: Promotion Branding Engine"""
        print("\nðŸŽ¨ Testing System 6: Promotion Branding Engine...")
        
        # Test 1: Create promotion branding
        ufc_branding = {
            "promotion_name": "UFC",
            "logo_url": "https://example.com/ufc-logo.png",
            "primary_color": "#D20A0A",
            "secondary_color": "#000000", 
            "accent_color": "#FFD700",
            "font_family": "Arial"
        }
        
        success1, response1 = self.run_test(
            "Create Promotion Branding - UFC",
            "POST",
            "branding/promotion",
            200,
            ufc_branding
        )
        
        if success1 and response1:
            print(f"   âœ… UFC branding created successfully")
            
            # Verify response structure
            required_fields = ['id', 'promotion_name', 'primary_color', 'secondary_color', 'accent_color', 'created_at']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âŒ Missing fields in branding response: {missing_fields}")
                return False
            
            print(f"   Promotion: {response1['promotion_name']}")
            print(f"   Primary Color: {response1['primary_color']}")
            print(f"   Secondary Color: {response1['secondary_color']}")
            print(f"   Accent Color: {response1['accent_color']}")
            print(f"   Font Family: {response1.get('font_family', 'N/A')}")
        
        # Test 2: Get existing promotion branding
        success2, response2 = self.run_test(
            "Get Promotion Branding - UFC",
            "GET",
            "branding/promotion/UFC",
            200
        )
        
        if success2 and response2:
            print(f"   âœ… UFC branding retrieved successfully")
            
            if response2.get('is_default') != False:
                print(f"   âŒ Expected is_default=False for existing branding")
                return False
            
            print(f"   Is Default: {response2.get('is_default')}")
            print(f"   Retrieved Promotion: {response2['promotion_name']}")
        
        # Test 3: Get non-existent promotion (should return defaults)
        success3, response3 = self.run_test(
            "Get Promotion Branding - Bellator (Non-existent)",
            "GET", 
            "branding/promotion/Bellator",
            200
        )
        
        if success3 and response3:
            print(f"   âœ… Default branding returned for non-existent promotion")
            
            if response3.get('is_default') != True:
                print(f"   âŒ Expected is_default=True for non-existent promotion")
                return False
            
            print(f"   Is Default: {response3.get('is_default')}")
            print(f"   Default Promotion: {response3['promotion_name']}")
            print(f"   Default Primary Color: {response3['primary_color']}")
        
        # Test 4: Update existing promotion branding
        updated_ufc_branding = {
            "promotion_name": "UFC",
            "logo_url": "https://example.com/ufc-logo-new.png",
            "primary_color": "#FF0000",
            "secondary_color": "#FFFFFF",
            "accent_color": "#FFAA00", 
            "font_family": "Roboto"
        }
        
        success4, response4 = self.run_test(
            "Update Promotion Branding - UFC",
            "POST",
            "branding/promotion", 
            200,
            updated_ufc_branding
        )
        
        if success4 and response4:
            print(f"   âœ… UFC branding updated successfully")
            
            # Verify updated values
            if response4['primary_color'] != updated_ufc_branding['primary_color']:
                print(f"   âŒ Primary color not updated correctly")
                return False
            
            if response4['font_family'] != updated_ufc_branding['font_family']:
                print(f"   âŒ Font family not updated correctly")
                return False
            
            print(f"   Updated Primary Color: {response4['primary_color']}")
            print(f"   Updated Font Family: {response4['font_family']}")
        
        return success1 and success2 and success3 and success4

    def test_broadcast_buffer(self):
        """Test System 7: Production Output Buffers"""
        print("\nðŸ“¡ Testing System 7: Production Output Buffers...")
        
        # Test 1: Configure broadcast buffer with 10 second delay
        buffer_config = {
            "bout_id": "test-bout-buffer-123",
            "delay_seconds": 10,
            "enabled": True
        }
        
        success1, response1 = self.run_test(
            "Configure Broadcast Buffer - 10s delay",
            "POST",
            "broadcast/buffer/config",
            200,
            buffer_config
        )
        
        if success1 and response1:
            print(f"   âœ… Broadcast buffer configured successfully")
            
            # Verify response structure
            required_fields = ['success', 'config']
            missing_fields = [field for field in required_fields if field not in response1]
            
            if missing_fields:
                print(f"   âŒ Missing fields in buffer config response: {missing_fields}")
                return False
            
            if response1['success'] != True:
                print(f"   âŒ Expected success=True")
                return False
            
            config = response1['config']
            print(f"   Bout ID: {config['bout_id']}")
            print(f"   Delay Seconds: {config['delay_seconds']}")
            print(f"   Enabled: {config['enabled']}")
            print(f"   Updated At: {config.get('updated_at', 'N/A')}")
        
        # Test 2: Get buffered data
        success2, response2 = self.run_test(
            "Get Buffered Data - 10s delay",
            "GET",
            "broadcast/buffer/test-bout-buffer-123",
            200
        )
        
        if success2 and response2:
            print(f"   âœ… Buffered data retrieved successfully")
            
            # Verify response structure
            required_fields = ['bout_id', 'delay_seconds', 'enabled', 'cutoff_time', 'message']
            missing_fields = [field for field in required_fields if field not in response2]
            
            if missing_fields:
                print(f"   âŒ Missing fields in buffered data response: {missing_fields}")
                return False
            
            if response2['delay_seconds'] != 10:
                print(f"   âŒ Expected delay_seconds=10, got {response2['delay_seconds']}")
                return False
            
            print(f"   Bout ID: {response2['bout_id']}")
            print(f"   Delay Seconds: {response2['delay_seconds']}")
            print(f"   Enabled: {response2['enabled']}")
            print(f"   Message: {response2['message']}")
        
        # Test 3: Configure different delay values
        test_configs = [
            {"bout_id": "test-bout-5s", "delay_seconds": 5, "enabled": True},
            {"bout_id": "test-bout-30s", "delay_seconds": 30, "enabled": True},
            {"bout_id": "test-bout-disabled", "delay_seconds": 10, "enabled": False}
        ]
        
        all_config_success = True
        for i, config in enumerate(test_configs):
            success, response = self.run_test(
                f"Configure Buffer - {config['delay_seconds']}s, enabled={config['enabled']}",
                "POST",
                "broadcast/buffer/config",
                200,
                config
            )
            
            if success and response:
                print(f"   âœ… Config {i+1} successful: {config['bout_id']}")
                
                if response['success'] != True:
                    print(f"   âŒ Config {i+1} failed: success != True")
                    all_config_success = False
            else:
                all_config_success = False
        
        # Test 4: Get buffer for non-existent bout (should return defaults)
        success4, response4 = self.run_test(
            "Get Buffer - Non-existent bout",
            "GET",
            "broadcast/buffer/non-existent-bout-999",
            200
        )
        
        if success4 and response4:
            print(f"   âœ… Default buffer returned for non-existent bout")
            
            # Should return default 5 second delay, enabled=True
            if response4['delay_seconds'] != 5:
                print(f"   âŒ Expected default delay_seconds=5, got {response4['delay_seconds']}")
                return False
            
            if response4['enabled'] != True:
                print(f"   âŒ Expected default enabled=True, got {response4['enabled']}")
                return False
            
            print(f"   Default Delay: {response4['delay_seconds']}s")
            print(f"   Default Enabled: {response4['enabled']}")
        
        return success1 and success2 and all_config_success and success4

    def test_phase2_phase3_mission_critical_systems(self):
        """Test all Phase 2 & 3 Mission-Critical Systems"""
        print("\nðŸŽ¯ Testing Phase 2 & 3 Mission-Critical Systems...")
        
        # Test System 3: Supervisor Dashboard Data Feeds
        print("\n" + "="*60)
        print("SYSTEM 3: SUPERVISOR DASHBOARD DATA FEEDS")
        print("="*60)
        success_system3 = self.test_supervisor_dashboard()
        
        # Test System 4: AI Judge Variance Detection
        print("\n" + "="*60)
        print("SYSTEM 4: AI JUDGE VARIANCE DETECTION")
        print("="*60)
        success_system4 = self.test_variance_detection()
        
        # Test System 6: Promotion Branding Engine
        print("\n" + "="*60)
        print("SYSTEM 6: PROMOTION BRANDING ENGINE")
        print("="*60)
        success_system6 = self.test_promotion_branding()
        
        # Test System 7: Production Output Buffers
        print("\n" + "="*60)
        print("SYSTEM 7: PRODUCTION OUTPUT BUFFERS")
        print("="*60)
        success_system7 = self.test_broadcast_buffer()
        
        # Summary
        print("\n" + "="*60)
        print("PHASE 2 & 3 MISSION-CRITICAL SYSTEMS SUMMARY")
        print("="*60)
        
        systems_results = {
            "System 3 - Supervisor Dashboard": success_system3,
            "System 4 - Variance Detection": success_system4,
            "System 6 - Promotion Branding": success_system6,
            "System 7 - Broadcast Buffers": success_system7
        }
        
        for system_name, success in systems_results.items():
            status = "âœ… PASSED" if success else "âŒ FAILED"
            print(f"   {system_name}: {status}")
        
        all_systems_passed = all(systems_results.values())
        
        if all_systems_passed:
            print(f"\nðŸŽ‰ ALL PHASE 2 & 3 MISSION-CRITICAL SYSTEMS PASSED! ðŸŽ‰")
        else:
            failed_systems = [name for name, success in systems_results.items() if not success]
            print(f"\nâŒ FAILED SYSTEMS: {', '.join(failed_systems)}")
        
        return all_systems_passed

    def run_all_tests(self):
        """Run all backend tests"""
        print("ðŸš€ Starting Combat Judging API Tests")
        print(f"Testing against: {self.base_url}")
        print("=" * 60)
        
        # Test basic connectivity
        self.test_root_endpoint()
        
        # Test status endpoints
        self.test_status_endpoints()
        
        # Test main scoring functionality
        self.test_calculate_score_endpoint()
        
        # Test edge cases
        self.test_edge_cases()
        
        # Test event counts in scoring API
        self.test_event_counts_in_scoring()
        
        # Test event counts with ACTUAL frontend event types
        self.test_event_counts_with_actual_frontend_types()
        
        # Test Control Timer Event Logging (NEW - 6 scenarios)
        self.test_control_timer_complete_flow()
        
        # Test Shadow Judging Training Mode (5 APIs)
        self.test_shadow_judging_complete_flow()
        
        # Test Security & Audit Feature (5 APIs)
        self.test_audit_integration_flow()
        
        # Test Judge Profile Management Feature (4 APIs)
        self.test_judge_profile_integration_flow()
        
        # Test Fighter Stats APIs (2 APIs)
        self.test_fighter_stats_integration_flow()
        
        # Test Discrepancy Flags APIs (3 APIs)
        self.test_discrepancy_flags_integration_flow()
        
        # Test Tuning Profiles APIs (5 APIs)
        self.test_tuning_profiles_integration_flow()
        
        # Test Round Notes Engine APIs (5 APIs) - NEW
        self.test_round_notes_complete_flow()
        
        # Test Phase 2 & 3 Mission-Critical Systems - NEW
        self.test_phase2_phase3_mission_critical_systems()
        
        # Print summary
        print("\n" + "=" * 60)
        print(f"ðŸ“Š Test Summary: {self.tests_passed}/{self.tests_run} tests passed")
        
        if self.tests_passed == self.tests_run:
            print("ðŸŽ‰ All tests passed!")
            return 0
        else:
            print("âŒ Some tests failed")
            return 1

    def get_results(self):
        """Get detailed test results"""
        return {
            'total_tests': self.tests_run,
            'passed_tests': self.tests_passed,
            'success_rate': (self.tests_passed / self.tests_run * 100) if self.tests_run > 0 else 0,
            'results': self.results
        }

def main():
    tester = CombatJudgingAPITester()
    exit_code = tester.run_all_tests()
    
    # Save results to file
    results = tester.get_results()
    with open('/app/backend_test_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return exit_code

if __name__ == "__main__":
    sys.exit(main())